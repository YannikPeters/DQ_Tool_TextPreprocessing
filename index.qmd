---
title: "Comparing Tools and Workflows for Data Quality in Basic Text Preprocessing with R - Part 1"
author: "Yannik Peters"
format: html
editor: visual
bibliography: references.bib
---

# 1. Introduction

The digitalisation has led to an innovation of research objects and research methods. While statistical methods to analyze numerical data have a long tradition, it is especially the automated analysis of text data that has seen huge improvement in recent years. Automated text analysis methods are applied to various data sources, be it social media data, news paper articles, parliamentary speeches, historical texts or literature. In this introduction, we want to focus on an important, necessary and often times challenging aspect related to data quality in text data: the text processing. Text processing can be defined as all changes that are done to the text data after the data collection and before the data analysis. Its main purpose is to bring the raw data in a form that is then suitable for applying specific research methods, but also to reduce the probability of errors. In this sense, text processing is heavily related to the measurement dimension of data quality. On the one hand, text processing can help to reduce measurement errors, by increasing consistency or accuracy. On the other hand, text processing itself can become a source for potential errors. In the TED-On, the "Total Error Framework for Digital Traces of Human Behavior On Online Platforms" [@sen2021] these errors are referred to as `trace reduction errors`. According to Sen et al. an example for this error would be: "Researchers might decide to use a classifier that removes spam, but has a high false positive rate, inadvertently removing non-spam tweets. They might likewise discard tweets without textual content, thereby ignoring relevant statements made through hyper-links or embedded pictures/videos" (p. 413).

In this tutorial we want to go through all the classic steps of text processing and compare as well as recommend different packages and tools to use. The final step is gonna be to create a document-feature-matrix (DFM) or more precise a document-term-matrix (DTM), that is used often times for multiple analysis techniques such as sentiment analysis or topic modeling. For this purpose, we created a small social media data set with posts about the Olympic summer games in Paris 2024. The Olympic summer games can be considered a transnational media event [@hepp2015], which is nowadays of course not only covered by traditional media but is communicatively accompanied on social media. For copyright reasons, we have constructed an artificial data set which does not contain any real content.

# 2. Set up

At first, we will open all relevant libraries. Please make sure to have all relevant packages installed using the `install.packages()` command. We will be using and comparing some of the most important text preprocessing and analysis R packages like `quanteda`, `stringr`, `textclean` or `tm`. We will also use specific packages like `skimr`, `spelling`, `polyglotr` or `deeplr` for very specific purposes.

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(quanteda)
library(textclean)
library(tm)
library(textTinyR)
library(skimr)
library(readr)
library(dplyr)
library(spelling) 
library(hunspell)
library(polyglotr)
library(deeplr)
library(text)
```

Finally, we will then load our artificial dataset from the Olympic summer games.

```{r message = FALSE, warning = FALSE}
olympics_data <- read_csv("6.modified_olympics_tweet.csv", locale = locale(encoding = "Latin1"))
olympics_data
```

# 3. Application of tools and use case

### 3.1 Basic data (quality) checks

Let's start with checking out the basic structure of our dataset.

```{r message = FALSE, warning = FALSE}
str(olympics_data)
```

Here we find 13 variables and 150 observations.

```{r message = FALSE, warning=FALSE}
summary(olympics_data)
```

When running the `summary()` function, the overview might get a bit messy depending on the size of the dataframe. We therefore recommend R tools, that offer basic data quality reports and arrange the results clearly like the `skimr` package. Here, you can receive an overview of the various variables of our data set including descriptive statistics and missing values.

```{r warning=FALSE}
skim(olympics_data)
```

### 3.2 Dealing with multiple languages

As the Olympic games are a transnational media event, it does not come as a surprise to receive a multilingual data set. The basic problem about multilingual data sets is that common CSS research methods like topic modeling or sentiment analysis are expected to be in one language. There are different strategies to deal with multilingual data sets. The chosen strategy has to depend on the specific contexts, the applied methods and the research design and questions. We can basically distinguish three or four main strategies with regard to multilingual data sets (see @haukelicht2023, @lind2021a). For this short version, we will discuss three main strategies.

1\) Selecting cases: one language

This approach is basically about choosing cases that only contain documents in one language. For our Twitter/X data set, we could for instance remove all postings which are not English ones.

```{r warning=FALSE, message=FALSE}
olympics_data_en <- olympics_data %>% filter(language == "en")
table(olympics_data_en$language)
```

Of course, this strategy might result in a representation error as we systematically exclude specific content from analysis (in our case twenty tweets). So let's take a look at the other strategies.

2\) Multiple single language data sets

Another way of dealing with multilingual data sets is to create language specific subsamples of our data. The main advantage of this strategy is, that we do not lose any content due to exclusion or translation errors. However, compared to the other methods there are more validation steps required with regard to the each single language data set (for detailed information see @haukelicht2023, @lind2021a). As we have already created a data set which only contains English tweets, we will create two more dataframes for German and French tweets.

```{r warning=FALSE, message=FALSE}
olympics_data_de <- olympics_data %>% filter(language == "de")
table(olympics_data_de$language)

olympics_data_fr <- olympics_data %>% filter(language == "fr")
table(olympics_data_fr$language)
```

We only find a few documents that are not in English resulting in some very small language sets. Therefore, this strategy might not be the best with regard to our example data.

3\) Translating

The third option of dealing with multilingual datasets is to translate the non-English speaking tweets into English. As this is a just a small, artificial, sample data set, we could actually translate the few tweets manually. In a real case scenario however, analyzing a data set of millions of tweets, you would usually use a automated translation algorithm or method. The main advantage of the translation method is to generate one singular data set, which can then be analyzed with one model only. This requires less resources all well. The main disadvantage lies in the potential of translation errors. It is therefore necessary to evaluate your translation method. For this purpose, let's translate all non-English comments with both tools in order to compare the results. The most common translation tools are `Google Translator` and `DeepL`. First, we will use the `polyglotr` and the `deeplr` package to translate the German text.

```{r message=FALSE, warning=FALSE}
#Translation of German posts and creation of translated dataframe using Google Translate
translation_google_de <- google_translate(olympics_data_de$tweet_text, target_language = "en", source_language = "de")
translation_google_de <- sapply(translation_google_de, function(x) x[[1]])
olympics_data_de_google <- olympics_data_de
olympics_data_de_google$tweet_text <- translation_google_de
```

To access the DeepL API, you currently need a developer account. You can use this [link](https://www.deepl.com/en/pro/change-plan#developer) to get to the registration page. A free account can translate up to 500,000 characters per month and gives you access to the DeepL Rest API. In order to translate our text using DeepL in R, you first need the API-key. When signing up for a developer account, you automatically receive such a key.

```{r message=FALSE, warning=FALSE, echo=FALSE}
my_key <- "e1b84858-e71d-4be5-a592-f67a6ffb35ea:fx"
```

```{r warning=FALSE, message=FALSE, results='hide'}
#Translation of German posts and creation of translated dataframe using DeepL
translation_deepl_de <- translate2(olympics_data_de$tweet_text, target_lang = "EN", auth_key = my_key)
olympics_data_de_deepl <- olympics_data_de
olympics_data_de_deepl$tweet_text <- translation_deepl_de
```

::: callout-note
For this code to work, make sure that you create a my_key object containing your API key.

my_key \<- "Your key"
:::

Let's compare the results for the German tweets.

```{r}
head(olympics_data_de_google$tweet_text)
head(olympics_data_de_deepl$tweet_text)
```

We can see from a quick comparison that the translations seem pretty similar. We can also use certain metric to determine the degree of similarity. For this example, we will apply [cosine similarity](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50). First, we have to `unlist` our text data, as the `COS_TEXT` function of the `textTidyR` package requires a vector as an input.

```{r}
google_translation_de <- unlist(translation_google_de)
deepl_translation_de <- unlist(translation_deepl_de)


cosine_similarities <- COS_TEXT(google_translation_de, deepl_translation_de, separator = " ")

cosine_similarities
mean(cosine_similarities)
```

With a mean cosine similarity of 0.82 on a scale from 0 to 1, the translations of Google Translate and DeepL are indeed very similar. We can do the very same for the French postings.

```{r message=FALSE, warning=FALSE}
#Translation of French posts and creation of translated dataframe using Google Translate
translation_google_fr <- google_translate(olympics_data_fr$tweet_text, target_language = "en", source_language = "fr")
translation_google_fr <- sapply(translation_google_fr, function(x) x[[1]])
olympics_data_fr_google <- olympics_data_fr
olympics_data_fr_google$tweet_text <- translation_google_fr

#Translation of French posts and creation of translated dataframe using DeepL
translation_deepl_fr <- translate2(olympics_data_fr$tweet_text, target_lang = "EN", auth_key = my_key)
olympics_data_fr_deepl <- olympics_data_fr
olympics_data_fr_deepl$tweet_text <- translation_deepl_fr

#compare Google Translate und Deepl translation manually
olympics_data_fr_google$tweet_text
olympics_data_fr_deepl$tweet_text

#unlist french translation data
google_translation_fr <- unlist(translation_google_fr)
deepl_translation_fr <- unlist(translation_deepl_fr)

#calculate cosine similarities for French translation data
cosine_similarities <- COS_TEXT(google_translation_fr, deepl_translation_fr, separator = " ")

cosine_similarities
mean(cosine_similarities)
```

With an average cosine similarity of 0.86, it is slightly higher for the French than the German translation.

The key question for us is now: Do we want to use the translation of DeepL or Google Translate? Generally, DeepL is considered to be more accurate then Google Translate (like [here](https://www.geeksforgeeks.org/deepl-vs-google/)). In current research, both tools are considered applicable. For the translation of Spanish idiomatic expressions into English, @hidalgo-ternero2021 find DeepL with an average accuracy rate of 89% slighty better than Google Translate with 86%. @sebo2024 do not find significant differences in the accuracy of both tools. One of the major advantages of Google Translate is that it can be applied to significantly more languages than DeepL. As we only have two languages to translate in our case, we will use the DeepL translation here.

```{r}
olympics_data_en_full <- rbind(olympics_data_en, olympics_data_de_deepl, olympics_data_fr_deepl)
```

::: callout-important
An alternative, fourth way would be to apply methods which are able to analyze multilingual text data. These methods are usually based on multilingual word or sentence embedding. Examples for these strategies can be found in @licht2023a or @chan2020.
:::

### 3.3 Minor text operations

Minor text operations and removing stopwords in text preprocessing are highly depended on two factors:

a\) The text data type

Compared to other types of text data, such as newspaper articles or literature, social media data often involves a rather informal communication style or consists of specific characteristics. For example, the proportion of abbreviations, slang, spelling mistakes or emojis is usually high, which can be considered as noise in the data.

b\) The specific method to be applied

Different methods in Computational Social Science require different strategies of text preprocessing. Methods that use a bag of words approach, for example, tend to remove a rather high number of different special characters, as these are not regarded as meaningful for interpretation, but rather as disruptive. In contrast, approaches that include context and semantics like modern transformer-based models tend to retain characteristics such as punctuation marks and require in general less preprocessing steps.

In our case, we will later apply a "classic" LDA. Therefore, we will apply some basic operations like removing hashtags, punctuaction or URLs.

First, we will remove all hashtags from the original text and save them in a separate column. We compared functions of multiple packages and decided to use the one from `textclean`, because the ones from the other packages performed slightly less accurate (f.e. some were not able to remove punctuation at the end of an hashtag within a sentence). To remove hashtags from the text is advisable for our case, as we analyze tweets which all contain #Olympics2024. In a topic model, #Olympics2024 would therefore be closely linked to every topic and not add significant value regarding interpretation.

```{r message=FALSE, warning=FALSE}
olympics_data_en_full <- olympics_data_en_full %>%
 mutate(
        	# Extract hashtags
              	Hashtags = sapply(str_extract_all(`tweet_text`, "#\\w+"), paste, collapse = " "),
        	
              	# Remove hashtags from the original text
            `tweet_text` = replace_hash(`tweet_text`, replacement = "")
      	) %>%
  	# Clean up any extra whitespace left after removing hashtags
      mutate(`tweet_text` = str_squish(`tweet_text`))

```

Besides removing the hashtags, we also want to remove special characters like URLs, punctuation or usernames as they do not add relevant information in BoW models like LDA, but rather increase the amount of words (tokens) to analyze and therefore the calculation time. Again, we will store the \@-mentions in a separate column to not lose information. As a last step of minor text operations, we want to set the whole text to lower cases in order to not treat the same word differently.

```{r message=FALSE, warning=FALSE}
# Create a new column for usernames and then clean the text
olympics_data_en_full <- olympics_data_en_full %>%
  mutate(
    #Store the usernames in a new column
    user_mentions = str_extract_all(tweet_text, "@\\w+") %>% 
      sapply(paste, collapse = ", "),  # Extract usernames with '@'
    
    # Clean the text
    tweet_text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_text) %>% #Remove retweets
      gsub("https?://\\S+", "", .) %>%  # Remove URLs
      gsub("@\\w+", "", .) %>%          # Remove @usernames from the text
      gsub("[\r\n]", " ", .) %>%        # Remove line breaks
      gsub("[[:punct:]]+", " ", .) %>%  # Remove punctuation
      gsub("\\bs\\b", " ", .) %>%       # Remove "s" as a standalone word
      gsub("\\bt\\b", " ", .) %>%       # Remove "t" as a standalone word
      gsub("\\s+", " ", .) %>%          # Reduce multiple spaces to a single space
      trimws(.) %>%                     # Trim whitespace from the beginning and end
      tolower()                         # Convert text to lowercase
  )

head(olympics_data_en_full$tweet_text)

```

### 3.4 Removing stopwords

### 3.5 Identifying misspelling

Now, we have created one data set with only English language postings. Next, we want to check is whether the text is correct in terms of spelling. Spelling errors are problematic with regard to text data quality as specific words might not be considered as equal to the correct word by specific methods. We therefore want to check the text data for errors. In R, there are many packages for spelling corrections.

```{r warning=FALSE, message=FALSE}
 
# Function to check spelling and return misspelled words with line numbers and corrections on the basis of hunspell dictionary

check_spelling_and_correct <- function(text) { 

# Use spell_check_text function from the spelling package 
misspelled <- spell_check_text(text) 

if (nrow(misspelled) > 0) { 

# Create a string of misspelled words with their positions 
misspelled_str <- paste(sapply(1:nrow(misspelled), function(i) { 

paste0(misspelled$word[i], " (", paste(misspelled$found[[i]], collapse = ","), ")") 
}), collapse = "; ") 


# Generate suggestions for misspelled words 

suggestions <- hunspell_suggest(misspelled$word) 

corrections <- sapply(suggestions, function(x) ifelse(length(x) > 0, x[1], NA)) 


# Create a string of corrections 

corrections_str <- paste(corrections, collapse = "; ") 

return(list(misspelled = misspelled_str, corrections = corrections_str)) 

} else { 

return(list(misspelled = NA_character_, corrections = NA_character_))}} 

# Apply the spelling check and correction to each tweet 

olympics_data_new <- olympics_data_en_full %>% 

mutate(Spelling_Check = lapply(tweet_text, check_spelling_and_correct), 

Misspelled_Words = sapply(Spelling_Check, function(x) x$misspelled), 

Corrected_Spellings = sapply(Spelling_Check, function(x) x$corrections)) %>% 

select(-Spelling_Check)  # Remove the intermediate column 

# View the results 

head(olympics_data_new[c("tweet_text", "Misspelled_Words", "Corrected_Spellings")]) 

```

### 3.6 Creating a DFM: tokenization and lemmatization

### 3.7 LDA Analysis

# 4. Discussion

# 5. Literature
