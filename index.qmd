---
title: "Comparing Tools and Workflows for Data Quality in Basic Text Preprocessing with R - Part 1"
author: "Yannik Peters"
format: html
editor: visual
bibliography: references.bib
---

# 1. Introduction

The digitalisation has led to an innovation of research objects and research methods. While statistical methods to analyze numerical data have a long tradition, it is especially the automated analysis of text data that has seen huge improvement in recent years. Automated text analysis methods are applied to various data sources, be it social media data, news paper articles, parliamentary speeches, historical texts or literature. In this introduction, we want to focus on an important, necessary and often times challenging aspect related to data quality in text data: the text processing. Text processing can be defined as all changes that are done to the text data after the data collection and before the data analysis. Its main purpose is to bring the raw data in a form that is then suitable for applying specific research methods, but also to reduce the probability of errors. In this sense, text processing is heavily related to the measurement dimension of data quality. On the one hand, text processing can help to reduce measurement errors, by increasing consistency or accuracy. On the other hand, text processing itself can become a source for potential errors. In the TED-On, the "Total Error Framework for Digital Traces of Human Behavior On Online Platforms" [@sen2021] these errors are referred to as `trace reduction errors`. According to Sen et al. an example for this error would be: "Researchers might decide to use a classifier that removes spam, but has a high false positive rate, inadvertently removing non-spam tweets. They might likewise discard tweets without textual content, thereby ignoring relevant statements made through hyper-links or embedded pictures/videos" (p. 413).

In this tutorial we want to go through all the classic steps of text processing and compare as well as recommend different packages and tools to use. The final step is gonna be to create a document-feature-matrix (DFM) or more precise a document-term-matrix (DTM), that is used often times for multiple analysis techniques such as sentiment analysis or topic modeling. For this purpose, we created a small social media data set with posts about the Olympic summer games in Paris 2024. The Olympic summer games can be considered a transnational media event [@hepp2015], which is nowadays of course not only covered by traditional media but is communicatively accompanied on social media. For copyright reasons, we have constructed an artificial data set which does not contain any real content.

# 2. Set up

At first, we will open all relevant libraries. Please make sure to have all relevant packages installed using the `install.packages()` command. We will be using and comparing some of the most important text preprocessing and analysis R packages like `quanteda`, `stringr`, `textclean` or `tm`. We will also use specific packages like `skimr`, `spelling`, `polyglotr` or `deeplr` for very specific purposes.

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(quanteda)
library(textclean)
library(tm)
library(textTinyR)
library(skimr)
library(readr)
library(dplyr)
library(spelling) 
library(hunspell)
library(polyglotr)
library(deeplr)
library(stringr)
library(stopwords)
```

Finally, we will then load our artificial dataset from the Olympic summer games.

```{r message = FALSE, warning = FALSE}
olympics_data <- read_csv("6.modified_olympics_tweet.csv", locale = locale(encoding = "Latin1"))
olympics_data
```

# 3. Application of tools and use case

### 3.1 Basic data (quality) checks

Let's start with checking out the basic structure of our dataset.

```{r message = FALSE, warning = FALSE}
str(olympics_data)
```

Here we find 13 variables and 150 observations.

```{r message = FALSE, warning=FALSE}
summary(olympics_data)
```

When running the `summary()` function, the overview might get a bit messy depending on the size of the dataframe. We therefore recommend R tools, that offer basic data quality reports and arrange the results clearly like the `skimr` package. Here, you can receive an overview of the various variables of our data set including descriptive statistics and missing values.

```{r warning=FALSE}
skim(olympics_data)
```

### 3.2 Dealing with multiple languages

As the Olympic games are a transnational media event, it does not come as a surprise to receive a multilingual data set. The basic problem about multilingual data sets is that common CSS research methods like topic modeling or sentiment analysis are expected to be in one language. There are different strategies to deal with multilingual data sets. The chosen strategy has to depend on the specific contexts, the applied methods and the research design and questions. We can basically distinguish three or four main strategies with regard to multilingual data sets (see @haukelicht2023, @lind2021a). For this short version, we will discuss three main strategies: 1) selecting cases, 2) multiple language data sets and 3) machine translation.

:::: callout-note
::: callout-important
An alternative, fourth way would be to apply methods which are able to analyze multilingual text data. These methods are usually based on multilingual word or sentence embedding. Examples for these strategies can be found in @licht2023a or @chan2020.
:::
::::

1\) Selecting cases: one language

This approach is basically about choosing cases that only contain documents in one language. For our Twitter/X data set, we could for instance remove all postings which are not English ones.

```{r warning=FALSE, message=FALSE}
olympics_data_en <- olympics_data %>% filter(language == "en")
table(olympics_data_en$language)
```

Of course, this strategy might result in a representation error as we systematically exclude specific content from analysis (in our case twenty tweets). So let's take a look at the other strategies.

2\) Multiple single language data sets

Another way of dealing with multilingual data sets is to create language specific subsamples of our data. The main advantage of this strategy is, that we do not lose any content due to exclusion or translation errors. However, compared to the other methods there are more validation steps required with regard to the each single language data set (for detailed information see @haukelicht2023, @lind2021a). As we have already created a data set which only contains English tweets, we will create two more dataframes for German and French tweets.

```{r warning=FALSE, message=FALSE}
olympics_data_de <- olympics_data %>% filter(language == "de")
table(olympics_data_de$language)

olympics_data_fr <- olympics_data %>% filter(language == "fr")
table(olympics_data_fr$language)
```

We only find a few documents that are not in English resulting in some very small language sets. Therefore, this strategy might not be the best with regard to our example data.

3\) (Machine) translating

The third option of dealing with multilingual datasets is to translate the non-English speaking tweets into English. As this is a just a small, artificial, sample data set, we could actually translate the few tweets manually. In a real case scenario however, analyzing a data set of millions of tweets, you would usually use a automated translation algorithm or method. The main advantage of the translation method is to generate one singular data set, which can then be analyzed with one model only. This requires less resources all well. The main disadvantage lies in the potential of translation errors. It is therefore necessary to evaluate your translation method. For this purpose, let's translate all non-English comments with both tools in order to compare the results. The most common translation tools are `Google Translator` and `DeepL`. First, we will use the `polyglotr` and the `deeplr` package to translate the German text.

```{r message=FALSE, warning=FALSE}
#Translation of German posts and creation of translated dataframe using Google Translate
translation_google_de <- google_translate(olympics_data_de$tweet_text, target_language = "en", source_language = "de")
translation_google_de <- sapply(translation_google_de, function(x) x[[1]])
olympics_data_de_google <- olympics_data_de
olympics_data_de_google$tweet_text <- translation_google_de
```

To access the DeepL API, you currently need a developer account. You can use this [link](https://www.deepl.com/en/pro/change-plan#developer) to get to the registration page. A free account can translate up to 500,000 characters per month and gives you access to the DeepL Rest API. In order to translate our text using DeepL in R, you first need the API-key. When signing up for a developer account, you automatically receive such a key.

```{r message=FALSE, warning=FALSE, echo=FALSE}
my_key <- "e1b84858-e71d-4be5-a592-f67a6ffb35ea:fx"
```

```{r warning=FALSE, message=FALSE, results='hide'}
#Translation of German posts and creation of translated dataframe using DeepL
translation_deepl_de <- translate2(olympics_data_de$tweet_text, target_lang = "EN", auth_key = my_key)
olympics_data_de_deepl <- olympics_data_de
olympics_data_de_deepl$tweet_text <- translation_deepl_de
```

::: callout-note
For this code to work, make sure that you create a my_key object containing your API key.

my_key \<- "Your key"
:::

Let's compare the results for the German tweets.

```{r}
head(olympics_data_de_google$tweet_text)
head(olympics_data_de_deepl$tweet_text)
```

We can see from a quick comparison that the translations seem pretty similar. We can also use certain metric to determine the degree of similarity. For this example, we will apply [cosine similarity](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50). First, we have to `unlist` our text data, as the `COS_TEXT` function of the `textTidyR` package requires a vector as an input.

```{r}
google_translation_de <- unlist(translation_google_de)
deepl_translation_de <- unlist(translation_deepl_de)


cosine_similarities_de <- COS_TEXT(google_translation_de, deepl_translation_de, separator = " ")

cosine_similarities_de
mean(cosine_similarities_de)
```

With a mean cosine similarity of 0.82 on a scale from 0 to 1, the translations of Google Translate and DeepL are indeed very similar. We can do the very same for the French postings.

```{r message=FALSE, warning=FALSE}
#Translation of French posts and creation of translated dataframe using Google Translate
translation_google_fr <- google_translate(olympics_data_fr$tweet_text, target_language = "en", source_language = "fr")
translation_google_fr <- sapply(translation_google_fr, function(x) x[[1]])
olympics_data_fr_google <- olympics_data_fr
olympics_data_fr_google$tweet_text <- translation_google_fr

#Translation of French posts and creation of translated dataframe using DeepL
translation_deepl_fr <- translate2(olympics_data_fr$tweet_text, target_lang = "EN", auth_key = my_key)
olympics_data_fr_deepl <- olympics_data_fr
olympics_data_fr_deepl$tweet_text <- translation_deepl_fr

#compare Google Translate und Deepl translation manually
olympics_data_fr_google$tweet_text
olympics_data_fr_deepl$tweet_text

#unlist french translation data
google_translation_fr <- unlist(translation_google_fr)
deepl_translation_fr <- unlist(translation_deepl_fr)

#calculate cosine similarities for French translation data
cosine_similarities_fr <- COS_TEXT(google_translation_fr, deepl_translation_fr, separator = " ")

cosine_similarities_fr
mean(cosine_similarities_fr)
```

With an average cosine similarity of 0.86, it is slightly higher for the French than the German translation.

The key question for us is now: Do we want to use the translation of DeepL or Google Translate? Generally, DeepL is considered to be more accurate then Google Translate (like [here](https://www.geeksforgeeks.org/deepl-vs-google/)). Still, Google Translate has proven to be very accurate with regard to calculating topic models [@devries2018]. In current research, both tools are considered applicable. For the translation of Spanish idiomatic expressions into English, @hidalgo-ternero2021 find DeepL with an average accuracy rate of 89% slighty better than Google Translate with 86%. @sebo2024 do not find significant differences in the accuracy of both tools. One of the major advantages of Google Translate is that it can be applied to significantly more languages than DeepL. As we only have two languages to translate in our case, we will use the DeepL translation here.

```{r}
olympics_data_en_full <- rbind(olympics_data_en, olympics_data_de_deepl, olympics_data_fr_deepl)
```

::: callout-important
Recent literature also highlighted the importance of using open source models for extrinsic data quality values like reproducibility [@chan2020], especially since they perform only slightly less accurate compared to the commercial ones [@licht2024a]. In R, the authors have currently not found any convincing open source alternative integrated in packages. Open source models like OpusMT can be used in Python via [Hugging Face](https://huggingface.co/models?pipeline_tag=translation&sort=trending). In R, these models can be accessed via the [`reticulate`](https://rstudio.github.io/reticulate/) package, which offers a connection to Python applications (see [here](https://rpubs.com/eR_ic/transfoRmers)). In future, a another option might be the [`text`](https://rpubs.com/eR_ic/transfoRmers) package, in which the translation function still has an experimental status.
:::

### 3.3 Minor text operations

Minor text operations and removing stopwords in text preprocessing are highly dependent on two factors:

a\) The text data type

Compared to other types of text data, such as newspaper articles or literature, social media data often involves a rather informal communication style or consists of specific characteristics. For example, the proportion of abbreviations, slang, spelling mistakes or emojis is usually high, which can be considered as noise in the data.

b\) The specific method to be applied

Different methods in Computational Social Science require different strategies of text preprocessing. Methods that use a bag of words approach, for example, tend to remove a rather high number of different special characters, as these are not regarded as meaningful for interpretation, but rather as disruptive. In contrast, approaches that include context and semantics like modern transformer-based models tend to retain characteristics such as punctuation marks and require in general less preprocessing steps.

In our case, we will later apply a "classic" LDA. Therefore, we will apply some basic operations like removing hashtags, punctuaction or URLs. Before we do so, let's check the lenght of our document.

```{r}
sum(str_count(olympics_data_en_full$tweet_text, '\\w+'))
```

First, we will remove all hashtags from the original text and save them in a separate column. We compared functions of multiple packages and decided to use the one from `textclean`, because the ones from the other packages performed slightly less accurate (f.e. some were not able to remove punctuation at the end of an hashtag within a sentence). To remove hashtags from the text is advisable for our case, as we analyze tweets which all contain #Olympics2024. In a topic model, #Olympics2024 would therefore be closely linked to every topic and not add significant value regarding interpretation.

```{r message=FALSE, warning=FALSE}
olympics_data_en_full <- olympics_data_en_full %>%
 mutate(
        	# Extract hashtags
              	Hashtags = sapply(str_extract_all(`tweet_text`, "#\\w+"), paste, collapse = " "),
        	
              	# Remove hashtags from the original text
            `tweet_text` = replace_hash(`tweet_text`, replacement = "")
      	) %>%
  	# Clean up any extra whitespace left after removing hashtags
      mutate(`tweet_text` = str_squish(`tweet_text`))

sum(str_count(olympics_data_en_full$tweet_text, '\\w+'))

```

Removing the hashtags has decreased our total word count by 691 words. Besides the hashtags, we also want to remove special characters like URLs, punctuation or usernames as they do not add relevant information in BoW models like LDA, but rather increase the amount of words (tokens) to analyze and therefore the calculation time. Again, we will store the \@-mentions in a separate column to not lose information. As a last step of minor text operations, we want to set the whole text to lower cases in order to not treat the same word differently.

```{r message=FALSE, warning=FALSE}
# Create a new column for usernames and then clean the text
olympics_data_en_full <- olympics_data_en_full %>%
  mutate(
    #Store the usernames in a new column
    user_mentions = str_extract_all(tweet_text, "@\\w+") %>% 
      sapply(paste, collapse = ", "),  # Extract usernames with '@'
    
    # Clean the text
    tweet_text = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweet_text) %>% #Remove retweets
      gsub("https?://\\S+", "", .) %>%  # Remove URLs
      gsub("@\\w+", "", .) %>%          # Remove @usernames from the text
      gsub("[\r\n]", " ", .) %>%        # Remove line breaks
      gsub("[[:punct:]]+", " ", .) %>%  # Remove punctuation
      gsub("\\s+", " ", .) %>%          # Reduce multiple spaces to a single space
      trimws(.) %>%                     # Trim whitespace from the beginning and end
      tolower()                         # Convert text to lowercase
  )

head(olympics_data_en_full$tweet_text)

sum(str_count(olympics_data_en_full$tweet_text, '\\w+'))

```

### 3.4 Removing stopwords

After doing the minor steps of text preprocessing, we now want to focus on removing stopwords as they can highly impact the outcome of certain models. Stopwords can be refered to as common words, often times frequently used, which add only little or no meaning for interpretation. Many popular text mining packages in R like `quanteda` offer stopword lists. Quite often these stopword lists are applied by default, while their specifics and peculiarities are not taken into account. For example, [@nothman2018]found a lot of "surprising omissions (e.g. *hasn't* but no *hadn't*) and inclusions (e.g. *computer*)". Also @hvitfeldt2021g found out inconsistencies in specific stopword lists. For example, the SMART stopword list like contains "he's" but not "she's". Therefore, it is worth checking for the impact of stopword lists with regard to your own data. In this tutorial, we will compare three commonly used and general stopword lists of the `stopwords` package: NLTK, SMART and Stopwords ISO. Even tough they overlap and can be integrated, `stopwords` contains a higher number of different lists compared to general text analysis packages with build-in stopword lists. Let's compare now the three lists following first the procedure of @hvitfeldt2021g.

```{r}
length(stopwords(source = "nltk"))
length(stopwords(source = "smart"))
length(stopwords(source = "stopwords-iso"))
```

It becomes clear, that the lists significantly differ in length. While NLPK is comparatively short, Stopwords ISO contains more than seven times as much words. Let's check now which words the three lists would exclude to see the differences.

```{r}
olympics_data_en_full_nltk <- olympics_data_en_full
olympics_data_en_full_smart <- olympics_data_en_full
olympics_data_en_full_iso <- olympics_data_en_full


extract_stopwords <- function(text, source) {
     # Get stopwords for the specified source
     stops <- stopwords(language = "en", source = source)
     # Split into words
     words <- text %>%
         strsplit("\\s+") %>%
         unlist()
     # Find intersection with stopwords
     found_stops <- intersect(words, stops)
     # Return as comma-separated string
     paste(found_stops, collapse = ", ")
 }
# Apply extraction for each source
olympics_data_en_full_nltk <- olympics_data_en_full_nltk %>%
     mutate(
         nltk_stopwords = sapply(tweet_text, extract_stopwords, source = "nltk"))

olympics_data_en_full_smart <- olympics_data_en_full_smart %>%
     mutate(
         smart_stopwords = sapply(tweet_text, extract_stopwords, source = "smart"))

olympics_data_en_full_iso <- olympics_data_en_full_iso %>%
     mutate(
         iso_stopwords = sapply(tweet_text, extract_stopwords, source = "stopwords-iso"))

sum(str_count(olympics_data_en_full_nltk$nltk_stopwords, '\\w+'))
sum(str_count(olympics_data_en_full_smart$smart_stopwords, '\\w+'))
sum(str_count(olympics_data_en_full_iso$iso_stopwords, '\\w+'))



```

As expected, we find for our artificial data set, Stopwords ISO would remove the highest number of words from our text. Even though it consists of more than twice as much words that SMART, the difference between the lists is only 81 words. The bigger gap can be found between SMART and NLTK. Of course, the reason for this lies in the fact of certain shared stopwords being highly represented in a lot of tweets, while some of the unique words of the respective lists occur rather seldom. Let's apply now the three lists and compare the similarity between the procrossed text columns.

```{r}

remove_stopwords <- function(data, text_column, stopword_source) {
  # Get stopwords from the stopwords package
  stops <- stopwords(language = "en", source = stopword_source)
  
  # Remove stopwords from the specified text column
  data[[text_column]] <- sapply(data[[text_column]], function(text) {
    words <- strsplit(text, "\\s+")[[1]]       # Split text into words
    filtered_words <- setdiff(words, stops)     # Remove stopwords
    paste(filtered_words, collapse = " ")       # Reassemble text without stopwords
  })
  
  return(data)
}

# Example usage
olympics_data_en_full_nltk <- remove_stopwords(olympics_data_en_full_nltk, "tweet_text", "nltk")
olympics_data_en_full_smart <- remove_stopwords(olympics_data_en_full_smart, "tweet_text", "smart")
olympics_data_en_full_iso <- remove_stopwords(olympics_data_en_full_iso, "tweet_text", "stopwords-iso")

# Calculating cosine similarity
cosine_similarities_nltk_smart <- COS_TEXT(olympics_data_en_full_nltk$tweet_text, olympics_data_en_full_smart$tweet_text, separator = " ")
cosine_similarities_nltk_iso <- COS_TEXT(olympics_data_en_full_nltk$tweet_text, olympics_data_en_full_iso$tweet_text, separator = " ")
cosine_similarities_smart_iso <- COS_TEXT(olympics_data_en_full_smart$tweet_text, olympics_data_en_full_iso$tweet_text, separator = " ")

mean(cosine_similarities_nltk_smart)
mean(cosine_similarities_nltk_iso)
mean(cosine_similarities_smart_iso)

```

As expected, the similarity between NLTK and ISO is lowest and highest between SMART and ISO. Even though the similarities are quite high, it becomes clear that using specific stopword lists leads to changes regarding your corpus and subsequently to your analysis. Therefore, we need to evaluate which lists works best with regard to our data set and use case as well as the method to be applied. For this reason, we will identify the words, which have been removed only for one specific list.

```{r}

# Combine the three stopword variables into one data frame
combined_df <- data.frame(nltk_stopwords = olympics_data_en_full_nltk$nltk_stopwords,
  smart_stopwords = olympics_data_en_full_smart$smart_stopwords,
  iso_stopwords = olympics_data_en_full_iso$iso_stopwords,
  stringsAsFactors = FALSE)

# Function to create a unique word list from a string
get_unique_words <- function(column) {
  unique(unlist(str_split(column, ",\\s*")))  # Split by commas and remove spaces Leerzeichen
}

# Create sets of unique words for each column
words_nltk <- get_unique_words(combined_df$nltk_stopwords)
words_smart <- get_unique_words(combined_df$smart_stopwords)
words_iso <- get_unique_words(combined_df$iso_stopwords)

# Words only in column nltk_stopwords
unique_nltk <- setdiff(words_nltk, union(words_smart, words_iso))

# Words only in column smart_stopwords
unique_smart <- setdiff(words_smart, union(words_nltk, words_iso))

# Words only in column iso_stopwords
unique_iso <- setdiff(words_iso, union(words_nltk, words_smart))

# Display the results
cat("Words only in nltk_stopwords:", unique_nltk, "\n")
cat("Words only in smart_stopwords:", unique_smart, "\n")
cat("Words only in iso_stopwords:", unique_iso, "\n")

```

It appears that there are only unique words removed for the ISO list. When inspecting the words, we find words which do have interpretative meaning regarding our #olympics24 dataset like results, world, top, proud, home, show, test or beginning. This indicates that Stopwords ISO is not the right list for our use case. Let's now compare now only NLTK and SMART in order to identify the best list for our use case. First, let's see which unique words are removed in NLTK but not in SMART and the opposite.

```{r}
get_unique_words_2 <- function(column) {
  unique(unlist(str_split(column, ",\\s*")))  # Split by commas and remove spaces Leerzeichen
}

# Create sets of unique words for each column
words_nltk <- get_unique_words(combined_df$nltk_stopwords)
words_smart <- get_unique_words(combined_df$smart_stopwords)

# Words only in column nltk_stopwords
unique_nltk <- setdiff(words_nltk, words_smart)

# Words only in column smart_stopwords
unique_smart <- setdiff(words_smart, words_nltk)

# Display the results
cat("Words only in nltk_stopwords:", unique_nltk, "\n")
cat("Words only in smart_stopwords:", unique_smart, "\n")

```

It becomes clear that SMART removes more words without any substantial meaning. Still there are some words includes which might be meaningful with regard to our use case like first, last, best or believe. With "ve", there is also one "word" which is removed from our text in NLTK but not in SMART. This is because SMART and NLTK have different strategies of dealing with contradictions.

```{r}
setdiff(stopwords(source = "nltk"),
        stopwords(source = "smart"))
```

NLTK also includes word fragments after removing punctuation like "don", "ll" or "ve". As we already have removed punctuation, we do need to include these forms. A good strategy could therefore be to construct an individualized stopword list on the basis of both lists. This would mean to include the unique NLTK words in SMART and remove words from SMART which are meaningful with regard to our content.

```{r}

```

What has already been true for minor text operations also counts for stopwords: the choice for a concrete stopword list should be taken with regard to the data, case study and method applied.

### 3.5 Identifying misspelling

Now, we have created one data set with only English language postings. Next, we want to check whether the text is correct in terms of spelling. Spelling errors are problematic with regard to text data quality as specific words might not be considered as equal to the correct word by specific methods. We therefore want to check the text data for errors. In R, there are many packages for spelling corrections.

```{r warning=FALSE, message=FALSE}
 
# Function to check spelling and return misspelled words with line numbers and corrections on the basis of hunspell dictionary

check_spelling_and_correct <- function(text) { 

# Use spell_check_text function from the spelling package 
misspelled <- spell_check_text(text) 

if (nrow(misspelled) > 0) { 

# Create a string of misspelled words with their positions 
misspelled_str <- paste(sapply(1:nrow(misspelled), function(i) { 

paste0(misspelled$word[i], " (", paste(misspelled$found[[i]], collapse = ","), ")") 
}), collapse = "; ") 


# Generate suggestions for misspelled words 

suggestions <- hunspell_suggest(misspelled$word) 

corrections <- sapply(suggestions, function(x) ifelse(length(x) > 0, x[1], NA)) 


# Create a string of corrections 

corrections_str <- paste(corrections, collapse = "; ") 

return(list(misspelled = misspelled_str, corrections = corrections_str)) 

} else { 

return(list(misspelled = NA_character_, corrections = NA_character_))}} 

# Apply the spelling check and correction to each tweet 

olympics_data_new <- olympics_data_en_full %>% 

mutate(Spelling_Check = lapply(tweet_text, check_spelling_and_correct), 

Misspelled_Words = sapply(Spelling_Check, function(x) x$misspelled), 

Corrected_Spellings = sapply(Spelling_Check, function(x) x$corrections)) %>% 

select(-Spelling_Check)  # Remove the intermediate column 

# View the results 

head(olympics_data_new[c("tweet_text", "Misspelled_Words", "Corrected_Spellings")]) 

```

### 3.6 Creating a DFM: tokenization and lemmatization

### 3.7 LDA Analysis

# 4. Discussion

# 5. Literature
