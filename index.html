<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yannik Peters, Kunjan Shah">

<title>TextPrep</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">TextPrep</h1>
<p class="subtitle lead">Comparing Tools and Workflows for Data Quality in Basic Text Preprocessing with R</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yannik Peters, Kunjan Shah </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="at-a-glance" class="level1">
<h1><strong>At a Glance</strong></h1>
<p>In this tutorial you will learn:</p>
<ol type="1">
<li><strong>Enhancing data quality through preprocessing:</strong> The tutorial provides a practical guide on how preprocessing methods, such as automated translation, minor text operations and stopword removal, can significantly improve the quality of social media data depended on use case, data types and methods.</li>
<li><strong>Comparison of tools, packages and strategies:</strong> By systematically evaluating and comparing different approaches (e.g.&nbsp;different stopword lists), it is highlighted how they can alter textual content and impact data interpretation and quality.</li>
<li><strong>Creation and analysis of preprocessing levels:</strong> Defining four preprocessing levels offers a structured framework to analyze text data at varying degrees of preparation, helping to understand how preprocessing affects analytical outcomes.</li>
<li><strong>Applying certain metrics to assess data quality:</strong> Text similarity measures such as word count or cosine similarity are used to document differences between the various preprocessing strategies and packages. Also Structural Topic Modeling is used to compare different preprocessing stages using semantic coherence and exclusivity.</li>
</ol>
</section>
<section id="introduction" class="level1">
<h1>1. Introduction</h1>
<p>The digitalisation has led to innovations of research objects and research methods. While statistical methods for analyzing numerical data have a long tradition, it is especially the automated analysis of text data that has seen a significant advancements in recent years. Automated text analysis methods are applied to various data sources, including social media data, news paper articles, parliamentary speeches, historical texts or literature. In this introduction, we want to focus on an important, necessary and often times challenging aspect related to data quality in text data: text preprocessing. Text preprocessing can be defined as all changes made to the text data after the data collection and before the data analysis. Its main purpose is to bring the raw data in a form that is then suitable for applying specific research methods, but also to reduce the likelihood of errors. In this sense, text processing is closely linked to the measurement dimension of data quality. On the one hand, text processing can help to reduce measurement errors, by increasing consistency or accuracy. On the other hand, text processing itself can become a source for potential errors. In the TED-On, the “Total Error Framework for Digital Traces of Human Behavior On Online Platforms” <span class="citation" data-cites="sen2021">(<a href="#ref-sen2021" role="doc-biblioref">Sen et al. 2021</a>)</span> these errors are referred to as <code>trace reduction errors</code>. According to Sen et al.&nbsp;an example for this error would be: “Researchers might decide to use a classifier that removes spam, but has a high false positive rate, inadvertently removing non-spam tweets. They might likewise discard tweets without textual content, thereby ignoring relevant statements made through hyper-links or embedded pictures/videos” (p.&nbsp;413).</p>
<p>In this tutorial we aim to explore different levels of text preprocessing and compare as well as recommend various packages, tools and functions to use. <span class="citation" data-cites="churchill2021">Churchill and Singh (<a href="#ref-churchill2021" role="doc-biblioref">2021</a>)</span> distinguish between four levels of text preprocessing.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Preprocessing level</th>
<th>Preprocessing operations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Elementary pattern-based preprocessing</td>
<td>e.g.&nbsp;removal of punctuaction, special characters, symbols, numbers etc.</td>
</tr>
<tr class="even">
<td>Dictionary-based preprocessing</td>
<td>e.g.&nbsp;removal of stopwords</td>
</tr>
<tr class="odd">
<td>Natural language preprocessing</td>
<td>e.g.&nbsp;stemming, lemmatization</td>
</tr>
<tr class="even">
<td>Statistical preprocessing</td>
<td>e.g.&nbsp;removal of tokens with high and low frequency</td>
</tr>
</tbody>
</table>
<p>We will apply operations from the first three preprocessing steps and evaluate how different choices for these steps impact our data. In this respect, data quality is not only a fixed condition but also a set of practices: “doing data quality”. In this tutorial, we emphasize the practice of comparison, i.e.&nbsp;determining which options of preprocessing influence the data (quality) and how. The types and sequence of text preprocessing steps depend on the nature of the data being analyzed and the methods applied. For instance, social media data tends to be noisier compared to more structured sources like newspaper articles or scientific papers. Additionally, the chosen analytical method significantly influences preprocessing requirements. Methods like LDA demand extensive preprocessing, while others, such as BERTopic, function effectively with minimal or no text modifications. In the final step of this tutorial, we will use differently preprocessed text data to apply Structural Topic Modeling (STM) <span class="citation" data-cites="roberts2019">(<a href="#ref-roberts2019" role="doc-biblioref">Roberts, Stewart, and Tingley 2019</a>)</span> and compare differences in the analysis. For this purpose, we created a small social media data set with posts about the Olympic summer games in Paris 2024, supposedly “collected” from the #Olympics2024. For copyright reasons, we have constructed an artificial data set that does not contain any real content. The Olympic summer games can be considered a transnational media event <span class="citation" data-cites="hepp2015">(<a href="#ref-hepp2015" role="doc-biblioref">Hepp 2015</a>)</span>, which today is, of course, not only covered by traditional media but is communicatively accompanied on social media.</p>
</section>
<section id="set-up" class="level1">
<h1>2. Set up</h1>
<p>First, we will load all relevant libraries. Please ensure to have all relevant packages installed using the <code>install.packages()</code> command. We will use and also compare functions from some of the most important text preprocessing and analysis R packages such as <code>quanteda</code>, <code>stringr</code>, <code>textclean</code> or <code>textTinyR</code>. Additionally, we will incorporate specific packages like <code>skimr</code>, <code>polyglotr</code> or <code>deeplr</code> for special purposes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(quanteda)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(textclean)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(textTinyR)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidytext)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(skimr)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(polyglotr)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(deeplr)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stopwords)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we will then load our artificial dataset from the Olympic summer games.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="st">"https://raw.githubusercontent.com/YannikPeters/DQ_Tool_TextPreprocessing/main/data/Olympics_Summer_1_2024.csv"</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>olympics_data <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(url, <span class="at">locale =</span> <span class="fu">locale</span>(<span class="at">encoding =</span> <span class="st">"Latin1"</span>))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>olympics_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 150 × 12
     no. tweet_id user_id timestamp   tweet_text subtopic language retweet_count
   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;
 1     1      226     139 09-08-2024… The pool … Swimmin… en                  12
 2     2    68257     133 31-07-2024… The fight… Doping … en                  47
 3     3    44351     197 06-08-2024… THE JOY O… Gold Me… en                  28
 4     4    95759     170 08-08-2024… Quel chem… Gold Me… fr                  34
 5     5    11173     106 27-07-2024… The inten… Wrestli… en                  46
 6     6    23334     177 28-07-2024… We must s… Doping … en                  39
 7     7    49688     160 04-08-2024… The culmi… Gold Me… en                   8
 8     8    39066     196 01-08-2024… Cheering … Maratho… en                  31
 9     9    81586     198 31-07-2024… Jede Leis… Athlete… de                   9
10    10    45338     134 03-08-2024… What a ga… Basketb… en                  16
# ℹ 140 more rows
# ℹ 4 more variables: like_count &lt;dbl&gt;, reply_count &lt;dbl&gt;, quote_count &lt;dbl&gt;,
#   source &lt;chr&gt;</code></pre>
</div>
</div>
</section>
<section id="application-of-tools-and-use-case" class="level1">
<h1>3. Application of tools and use case</h1>
<section id="basic-data-quality-checks" class="level3">
<h3 class="anchored" data-anchor-id="basic-data-quality-checks">3.1 Basic data (quality) checks</h3>
<p>Let’s start with checking out the basic structure of our data set.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(olympics_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>spc_tbl_ [150 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
 $ no.          : num [1:150] 1 2 3 4 5 6 7 8 9 10 ...
 $ tweet_id     : num [1:150] 226 68257 44351 95759 11173 ...
 $ user_id      : num [1:150] 139 133 197 170 106 177 160 196 198 134 ...
 $ timestamp    : chr [1:150] "09-08-2024 20:14" "31-07-2024 05:42" "06-08-2024 14:25" "08-08-2024 06:41" ...
 $ tweet_text   : chr [1:150] "The pool is alive with excitement! Who will claim victory in #SwimmingOlympics? The competition is fierce! http"| __truncated__ "The fight against doping is ongoing! Let's support our athletes by promoting #CleanSport at #Olympics2024. #Dop"| __truncated__ "THE JOY OF WINNING A #GOLDMEDAL IS UNMATCHED! http://www.jones.com/ WHO ARE YOU SUPPORTING AT #OLYMPICS2024? #O"| __truncated__ "Quel chemin parcouru ! Les gagnants de la Gold Medal nous ont tous rendus fiers ! #Olympics2024" ...
 $ subtopic     : chr [1:150] "Swimming Events" "Doping Awareness" "Gold Medal Moments" "Gold Medal Moments" ...
 $ language     : chr [1:150] "en" "en" "en" "fr" ...
 $ retweet_count: num [1:150] 12 47 28 34 46 39 8 31 9 16 ...
 $ like_count   : num [1:150] 5 54 4 40 51 52 20 93 44 30 ...
 $ reply_count  : num [1:150] 7 2 10 0 7 7 0 2 1 7 ...
 $ quote_count  : num [1:150] 1 2 8 0 5 9 8 2 6 2 ...
 $ source       : chr [1:150] "Twitter Web App" "Twitter Web App" "Twitter for iPhone" "Twitter for iPhone" ...
 - attr(*, "spec")=
  .. cols(
  ..   no. = col_double(),
  ..   tweet_id = col_double(),
  ..   user_id = col_double(),
  ..   timestamp = col_character(),
  ..   tweet_text = col_character(),
  ..   subtopic = col_character(),
  ..   language = col_character(),
  ..   retweet_count = col_double(),
  ..   like_count = col_double(),
  ..   reply_count = col_double(),
  ..   quote_count = col_double(),
  ..   source = col_character()
  .. )
 - attr(*, "problems")=&lt;externalptr&gt; </code></pre>
</div>
</div>
<p>Here we find 13 variables and 150 observations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(olympics_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      no.            tweet_id        user_id       timestamp        
 Min.   :  1.00   Min.   :  226   Min.   :100.0   Length:150        
 1st Qu.: 38.25   1st Qu.:24485   1st Qu.:128.0   Class :character  
 Median : 75.50   Median :49100   Median :149.5   Mode  :character  
 Mean   : 75.50   Mean   :50367   Mean   :151.1                     
 3rd Qu.:112.75   3rd Qu.:73639   3rd Qu.:177.0                     
 Max.   :150.00   Max.   :99451   Max.   :200.0                     
  tweet_text          subtopic           language         retweet_count  
 Length:150         Length:150         Length:150         Min.   : 0.00  
 Class :character   Class :character   Class :character   1st Qu.:14.25  
 Mode  :character   Mode  :character   Mode  :character   Median :28.00  
                                                          Mean   :26.85  
                                                          3rd Qu.:40.00  
                                                          Max.   :50.00  
   like_count     reply_count     quote_count        source         
 Min.   : 0.00   Min.   : 0.00   Min.   : 0.000   Length:150        
 1st Qu.:17.25   1st Qu.: 3.00   1st Qu.: 3.000   Class :character  
 Median :38.50   Median : 7.00   Median : 5.000   Mode  :character  
 Mean   :40.69   Mean   : 6.86   Mean   : 5.207                     
 3rd Qu.:61.75   3rd Qu.:10.00   3rd Qu.: 8.000                     
 Max.   :99.00   Max.   :15.00   Max.   :10.000                     </code></pre>
</div>
</div>
<p>When running the <code>summary()</code> function, the overview might become difficult to interpret depending on the size of the dataframe. Therefore, we recommend R tools that provide basic data quality reports and present the results in a clearer format, such as the <code>skimr</code> package. With skimr, you can obtain an overview of the various variables of our data set, including descriptive statistics and missing values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>skimr<span class="sc">::</span><span class="fu">skim</span>(olympics_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Data summary</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">Name</td>
<td style="text-align: left;">olympics_data</td>
</tr>
<tr class="even">
<td style="text-align: left;">Number of rows</td>
<td style="text-align: left;">150</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Number of columns</td>
<td style="text-align: left;">12</td>
</tr>
<tr class="even">
<td style="text-align: left;">_______________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Column type frequency:</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">character</td>
<td style="text-align: left;">5</td>
</tr>
<tr class="odd">
<td style="text-align: left;">numeric</td>
<td style="text-align: left;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">________________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Group variables</td>
<td style="text-align: left;">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table class="caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 19%">
<col style="width: 13%">
<col style="width: 19%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: right;">min</th>
<th style="text-align: right;">max</th>
<th style="text-align: right;">empty</th>
<th style="text-align: right;">n_unique</th>
<th style="text-align: right;">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">timestamp</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">150</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">tweet_text</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">73</td>
<td style="text-align: right;">171</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">145</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">subtopic</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">language</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">source</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table class="caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 14%">
<col style="width: 10%">
<col style="width: 14%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 4%">
<col style="width: 9%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 6%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
<th style="text-align: right;">p0</th>
<th style="text-align: right;">p25</th>
<th style="text-align: right;">p50</th>
<th style="text-align: right;">p75</th>
<th style="text-align: right;">p100</th>
<th style="text-align: left;">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">no.</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">75.50</td>
<td style="text-align: right;">43.45</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">38.25</td>
<td style="text-align: right;">75.5</td>
<td style="text-align: right;">112.75</td>
<td style="text-align: right;">150</td>
<td style="text-align: left;">▇▇▇▇▇</td>
</tr>
<tr class="even">
<td style="text-align: left;">tweet_id</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">50367.45</td>
<td style="text-align: right;">28499.46</td>
<td style="text-align: right;">226</td>
<td style="text-align: right;">24485.00</td>
<td style="text-align: right;">49099.5</td>
<td style="text-align: right;">73639.00</td>
<td style="text-align: right;">99451</td>
<td style="text-align: left;">▇▇▇▇▇</td>
</tr>
<tr class="odd">
<td style="text-align: left;">user_id</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">151.09</td>
<td style="text-align: right;">29.09</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">128.00</td>
<td style="text-align: right;">149.5</td>
<td style="text-align: right;">177.00</td>
<td style="text-align: right;">200</td>
<td style="text-align: left;">▆▇▆▇▆</td>
</tr>
<tr class="even">
<td style="text-align: left;">retweet_count</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">26.85</td>
<td style="text-align: right;">14.50</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">14.25</td>
<td style="text-align: right;">28.0</td>
<td style="text-align: right;">40.00</td>
<td style="text-align: right;">50</td>
<td style="text-align: left;">▆▆▇▇▇</td>
</tr>
<tr class="odd">
<td style="text-align: left;">like_count</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">40.69</td>
<td style="text-align: right;">26.16</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">17.25</td>
<td style="text-align: right;">38.5</td>
<td style="text-align: right;">61.75</td>
<td style="text-align: right;">99</td>
<td style="text-align: left;">▇▇▇▆▂</td>
</tr>
<tr class="even">
<td style="text-align: left;">reply_count</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">6.86</td>
<td style="text-align: right;">4.61</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">3.00</td>
<td style="text-align: right;">7.0</td>
<td style="text-align: right;">10.00</td>
<td style="text-align: right;">15</td>
<td style="text-align: left;">▇▅▆▃▅</td>
</tr>
<tr class="odd">
<td style="text-align: left;">quote_count</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">5.21</td>
<td style="text-align: right;">3.12</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">3.00</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">8.00</td>
<td style="text-align: right;">10</td>
<td style="text-align: left;">▇▇▇▅▇</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="handling-multilingual-text-data" class="level3">
<h3 class="anchored" data-anchor-id="handling-multilingual-text-data">3.2 Handling multilingual text data</h3>
<p>As the Olympic games are a transnational media event, it is not surprising to receive a multilingual data set. The primary challgenge with multilingual data sets is that common CSS research methods, such as topic modeling or sentiment analysis typically expect text data to be in a single language.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you use this tutorial on your own data set and it is not a multilingual one, you can skip this part and continue with 3.3.</p>
</div>
</div>
<p>There are various strategies to handle multilingual data sets. The chosen strategy should depend on the specific context, the methods applied and the research design and questions. We can distinguish three or four main strategies for working with multilingual data sets (see <span class="citation" data-cites="haukelicht2023">Hauke Licht and Fabienne Lind (<a href="#ref-haukelicht2023" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="lind2021a">Lind et al. (<a href="#ref-lind2021a" role="doc-biblioref">2021</a>)</span>). For this short version, we will discuss three primary strategies: 1) selecting cases, 2) working with multiple language data sets and 3) using machine translation.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>An alternative, fourth way would be to apply methods which are able to analyze multilingual text data. These methods are usually based on multilingual word or sentence embedding. Examples for these strategies can be found in <span class="citation" data-cites="licht2023a">Licht (<a href="#ref-licht2023a" role="doc-biblioref">2023</a>)</span> or <span class="citation" data-cites="chan2020">Chan et al. (<a href="#ref-chan2020" role="doc-biblioref">2020</a>)</span>.</p>
</div>
</div>
<p>1) Selecting cases: a single language</p>
<p>This approach involves selecting cases that contain documents in only one language. For our Twitter/X data set, we could for instance remove all postings that are not in English.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>olympics_data_en <span class="ot">&lt;-</span> olympics_data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(language <span class="sc">==</span> <span class="st">"en"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(olympics_data_en<span class="sc">$</span>language)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 en 
127 </code></pre>
</div>
</div>
<p>Of course, this strategy might lead to a representation error as specific content is systematically excluded from analysis (in our case twenty tweets). Therefore, let’s explore the other strategies.</p>
<p>2) Multiple single language data sets</p>
<p>Another way of dealing with multilingual data sets is to create language-specific subsamples of our data. The main advantage of this strategy is, that no content is lost due to exclusion or translation errors. However, compared to the other methods there are more validation steps required for each single language data set (for detailed information see <span class="citation" data-cites="haukelicht2023">Hauke Licht and Fabienne Lind (<a href="#ref-haukelicht2023" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="lind2021a">Lind et al. (<a href="#ref-lind2021a" role="doc-biblioref">2021</a>)</span>). As we have already created a data set which only contains English tweets, we will create two additional dataframes for German and French tweets.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>olympics_data_de <span class="ot">&lt;-</span> olympics_data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(language <span class="sc">==</span> <span class="st">"de"</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(olympics_data_de<span class="sc">$</span>language)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
de 
13 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>olympics_data_fr <span class="ot">&lt;-</span> olympics_data <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(language <span class="sc">==</span> <span class="st">"fr"</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(olympics_data_fr<span class="sc">$</span>language)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
fr 
10 </code></pre>
</div>
</div>
<p>We only find a few documents that are not in English resulting in very small language subsets. Therefore, this strategy might not be the best for our example data.</p>
<p>3) (Machine) Translation</p>
<p>The third option of dealing with multilingual datasets is to translate the non-English speaking tweets into English. Since this is a just a small, artificial, sample data set, we could actually translate the few tweets manually. In a real case scenario however, analyzing a data set of millions of tweets, you would typically use an automated translation algorithm or method. The main advantage of the translation method is to generate one singular data set, which can then be analyzed using one model. This approach also requires fewer resources. The main disadvantage lies in the potential for translation errors. It is therefore necessary to evaluate the translation method used. For this purpose, let’s translate all non-English tweets with both tools in order to compare the results. The most common translation tools are <code>Google Translator</code> and <code>DeepL</code>. First, we will use the <code>polyglotr</code> and <code>deeplr</code> packages to translate the German text.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Translation of German posts and creation of translated dataframe using Google Translate</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>translation_google_de <span class="ot">&lt;-</span> polyglotr<span class="sc">::</span><span class="fu">google_translate</span>(olympics_data_de<span class="sc">$</span>tweet_text, <span class="at">target_language =</span> <span class="st">"en"</span>, <span class="at">source_language =</span> <span class="st">"de"</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>translation_google_de <span class="ot">&lt;-</span> <span class="fu">sapply</span>(translation_google_de, <span class="cf">function</span>(x) x[[<span class="dv">1</span>]])</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>olympics_data_de_google <span class="ot">&lt;-</span> olympics_data_de</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>olympics_data_de_google<span class="sc">$</span>tweet_text <span class="ot">&lt;-</span> translation_google_de</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To access the DeepL API, you need a developer account. You can use this <a href="https://www.deepl.com/en/pro/change-plan#developer">link</a> to reach the registration page. A free account allows you to translate up to 500,000 characters per month and provides access to the DeepL Rest API. To translate text data using DeepL in R, you first need the API-key. When signing up for a developer account, you will automatically receive this key.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Translation of German posts and creation of translated dataframe using DeepL</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>translation_deepl_de <span class="ot">&lt;-</span> deeplr<span class="sc">::</span><span class="fu">translate2</span>(olympics_data_de<span class="sc">$</span>tweet_text, <span class="at">target_lang =</span> <span class="st">"EN"</span>, <span class="at">auth_key =</span> my_key)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>olympics_data_de_deepl <span class="ot">&lt;-</span> olympics_data_de</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>olympics_data_de_deepl<span class="sc">$</span>tweet_text <span class="ot">&lt;-</span> translation_deepl_de</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For this code to work, make sure that you create a my_key object containing your API key.</p>
<p>my_key &lt;- “Your key”</p>
</div>
</div>
<p>Let’s compare the results for the German tweets.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(olympics_data_de_google<span class="sc">$</span>tweet_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Every performance at Olympics24 underlines the hard work and commitment of our athletes! #Olympics2024"                           
[2] "What a journey it was! The gold medalists made us all proud! #Olympics2024"                                                       
[3] "Integrity in sport is important! Let's promote fair competition and say no to doping! #Olympics2024"                              
[4] "History is being made at the Olympic Swimming Games! Every stroke counts as athletes dive for glory! #Olympics2024"               
[5] "It's incredible what's possible in basketball! Who is your favorite team? #Olympics2024"                                          
[6] "The dedication of the athletes to their sport is admirable! The 2024 Olympic Games are a platform for great things! #Olympics2024"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(olympics_data_de_deepl<span class="sc">$</span>tweet_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Every performance at Olympics24 highlights the hard work and dedication of our athletes! #Olympics2024"                     
[2] "What a journey it was! The gold medal winners have made us all proud! #Olympics2024"                                        
[3] "Integrity in sport is important! Let's promote fair competition and say no to doping! #Olympics2024"                        
[4] "History is made at the Swimming Olympics! Every stroke counts as athletes dive for glory! #Olympics2024"                    
[5] "It's unbelievable what's possible in basketball! Who is your favorite team? #Olympics2024"                                  
[6] "The athletes' dedication to their sport is admirable! The 2024 Olympic Games are a platform for great things! #Olympics2024"</code></pre>
</div>
</div>
<p>A quick comparison shows that the translations seem pretty similar. We can also use certain metrics to determine the degree of similarity. For this example, we will apply <a href="https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50">cosine similarity</a>. First, we need to <code>unlist</code> our text data, as the <code>COS_TEXT</code> function from the <code>textTinyR</code> package requires a vector as an input.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>google_translation_de <span class="ot">&lt;-</span> <span class="fu">unlist</span>(translation_google_de)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>deepl_translation_de <span class="ot">&lt;-</span> <span class="fu">unlist</span>(translation_deepl_de)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>cosine_similarities_de <span class="ot">&lt;-</span> textTinyR<span class="sc">::</span><span class="fu">COS_TEXT</span>(google_translation_de, deepl_translation_de, <span class="at">separator =</span> <span class="st">" "</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>cosine_similarities_de</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.8571429 0.8593378 1.0000000 0.8838835 0.9166667 0.9100315 1.0000000
 [8] 0.9058216 0.8839600 0.9655172 0.9233805 0.7825856 0.9285714</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cosine_similarities_de)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9089922</code></pre>
</div>
</div>
<p>With a mean cosine similarity of 0.90 on a scale from 0 to 1, the translations from Google Translate and DeepL are indeed very similar. We can apply the same process for the French postings.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Translation of French posts and creation of translated dataframe using Google Translate</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>translation_google_fr <span class="ot">&lt;-</span> polyglotr<span class="sc">::</span><span class="fu">google_translate</span>(olympics_data_fr<span class="sc">$</span>tweet_text, <span class="at">target_language =</span> <span class="st">"en"</span>, <span class="at">source_language =</span> <span class="st">"fr"</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>translation_google_fr <span class="ot">&lt;-</span> <span class="fu">sapply</span>(translation_google_fr, <span class="cf">function</span>(x) x[[<span class="dv">1</span>]])</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>olympics_data_fr_google <span class="ot">&lt;-</span> olympics_data_fr</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>olympics_data_fr_google<span class="sc">$</span>tweet_text <span class="ot">&lt;-</span> translation_google_fr</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Translation of French posts and creation of translated dataframe using DeepL</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>translation_deepl_fr <span class="ot">&lt;-</span> deeplr<span class="sc">::</span><span class="fu">translate2</span>(olympics_data_fr<span class="sc">$</span>tweet_text, <span class="at">target_lang =</span> <span class="st">"EN"</span>, <span class="at">auth_key =</span> my_key)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>olympics_data_fr_deepl <span class="ot">&lt;-</span> olympics_data_fr</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>olympics_data_fr_deepl<span class="sc">$</span>tweet_text <span class="ot">&lt;-</span> translation_deepl_fr</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co">#compare Google Translate und Deepl translation manually</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>olympics_data_fr_google<span class="sc">$</span>tweet_text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "What a journey! The Gold Medal winners made us all proud! #Olympics2024"                                                   
 [2] "Integrity in sport matters! Let's promote fair competition and say no to doping! #Olympics2024"                            
 [3] "Every Takedown Counts! The competitive spirit is alive at Wrestling2024! #Olympics2024"                                    
 [4] "What a journey we have come! Join us for the Closing Ceremony to relive these magical moments. #Olympics2024"              
 [5] "The teamwork in basketball is incredible! Go for gold! #Olympics2024"                                                      
 [6] "A race of endurance and spirit! Who will win the Paris Marathon? #Olympics2024"                                            
 [7] "The countdown to the fantastic Welcome Ceremony has begun! Are you ready to witness a spectacular spectacle? #Olympics2024"
 [8] "What a display of talent! These athletes are making their mark at the 2024 Olympics! #Olympics2024"                        
 [9] "Celebrating the athletes who made their dreams come true by winning gold medals! #Olympics2024"                            
[10] "Exciting times await us in basketball at the 2024 Olympics! Let the Games begin! #Olympics2024"                            </code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>olympics_data_fr_deepl<span class="sc">$</span>tweet_text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "What a long way we've come! The Gold Medal winners have made us all proud! #Olympics2024"                
 [2] "Integrity in sport is important! Let's promote fair competition and say no to doping! #Olympics2024"     
 [3] "Every Takedown counts! The spirit of competition is alive and well at Wrestling2024! #Olympics2024"      
 [4] "We've come a long way! Join us for the Closing Ceremony to relive these magical moments. #Olympics2024"  
 [5] "The teamwork that goes into basketball is incredible! Go for the gold! #Olympics2024"                    
 [6] "A race of endurance and spirit! Who will win the Paris Marathon? #Olympics2024"                          
 [7] "The countdown to the fantastic welcome ceremony has begun! Are you ready for a great show? #Olympics2024"
 [8] "What a demonstration of talent! These athletes are making their mark at the 2024 Olympics! #Olympics2024"
 [9] "Let's celebrate the athletes who made their dreams come true by winning gold medals!  #Olympics2024"     
[10] "Exciting times await us in basketball at the 2024 Olympics! Let the Games begin! #Olympics2024"          </code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co">#unlist french translation data</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>google_translation_fr <span class="ot">&lt;-</span> <span class="fu">unlist</span>(translation_google_fr)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>deepl_translation_fr <span class="ot">&lt;-</span> <span class="fu">unlist</span>(translation_deepl_fr)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="co">#calculate cosine similarities for French translation data</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>cosine_similarities_fr <span class="ot">&lt;-</span> textTinyR<span class="sc">::</span><span class="fu">COS_TEXT</span>(google_translation_fr, deepl_translation_fr, <span class="at">separator =</span> <span class="st">" "</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>cosine_similarities_fr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.7938566 0.8970852 0.7252407 0.7431605 0.7893522 1.0000000 0.7050240
 [8] 0.9375000 0.8970852 1.0000000</code></pre>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cosine_similarities_fr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.8488304</code></pre>
</div>
</div>
<p>With an average cosine similarity of 0.85, the French translation is slightly less similar than the German one.</p>
<p>If we combine the translated data sets with the original English language data set and then compare the cosine similarity, it is of course higher with 0,98. So in this case, the difference of our combined data set is minimal. This would, of course, be different if the amount of non-English content were higher.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>olympics_data_en_deepl_full <span class="ot">&lt;-</span> <span class="fu">rbind</span>(olympics_data_en, olympics_data_de_deepl, olympics_data_fr_deepl)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>olympics_data_en_google_full <span class="ot">&lt;-</span> <span class="fu">rbind</span>(olympics_data_en, olympics_data_de_google, olympics_data_fr_google)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>cosine_similarities_full <span class="ot">&lt;-</span> textTinyR<span class="sc">::</span><span class="fu">COS_TEXT</span>(olympics_data_en_deepl_full<span class="sc">$</span>tweet_text, olympics_data_en_google_full<span class="sc">$</span>tweet_text, <span class="at">separator =</span> <span class="st">" "</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cosine_similarities_full)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9820347</code></pre>
</div>
</div>
<p>The key question for us is now: Should we use the translation of DeepL or Google Translate? Generally, DeepL is considered to be more accurate than Google Translate (e.g.&nbsp;<a href="https://www.geeksforgeeks.org/deepl-vs-google/">here</a>). Still, Google Translate has proven to be highly accurate for calculating topic models <span class="citation" data-cites="devries2018">(<a href="#ref-devries2018" role="doc-biblioref">Vries, Schoonvelde, and Schumacher 2018</a>)</span>. In current research, both tools are considered suitable. For the translation of Spanish idiomatic expressions into English, <span class="citation" data-cites="hidalgo-ternero2021">Hidalgo-Ternero (<a href="#ref-hidalgo-ternero2021" role="doc-biblioref">2021</a>)</span> found DeepL to perform slightly better, with an average accuracy rate of 89%, than Google Translate at 86%. <span class="citation" data-cites="sebo2024">Sebo and Lucia (<a href="#ref-sebo2024" role="doc-biblioref">2024</a>)</span> did not find significant differences in the accuracy of the two tools. One of the major advantages of Google Translate is that it can be applied to significantly more languages than DeepL. As we had to translate only two languages in our example data, we will use DeepL for this example.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>url_2 <span class="ot">&lt;-</span> <span class="st">"https://raw.githubusercontent.com/YannikPeters/DQ_Tool_TextPreprocessing/main/data/olympics_data_en_deepl_full.csv"</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(url_2, <span class="at">locale =</span> <span class="fu">locale</span>(<span class="at">encoding =</span> <span class="st">"Latin1"</span>))</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 150 × 12
     no. tweet_id user_id timestamp   tweet_text subtopic language retweet_count
   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;
 1     1      226     139 09-08-2024… The pool … Swimmin… en                  12
 2     2    68257     133 31-07-2024… The fight… Doping … en                  47
 3     3    44351     197 06-08-2024… THE JOY O… Gold Me… en                  28
 4     5    11173     106 27-07-2024… The inten… Wrestli… en                  46
 5     6    23334     177 28-07-2024… We must s… Doping … en                  39
 6     7    49688     160 04-08-2024… The culmi… Gold Me… en                   8
 7     8    39066     196 01-08-2024… Cheering … Maratho… en                  31
 8    10    45338     134 03-08-2024… What a ga… Basketb… en                  16
 9    11    24439     117 11-08-2024… We must s… Doping … en                  11
10    12    32959     110 06-08-2024… The spiri… Maratho… en                  44
# ℹ 140 more rows
# ℹ 4 more variables: like_count &lt;dbl&gt;, reply_count &lt;dbl&gt;, quote_count &lt;dbl&gt;,
#   source &lt;chr&gt;</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The translations of DeepL and Google Translate may differ slightly with each new API request due to coincidence or updates to the model. Especially when using Bag of Word models, this can lead to deviations depending on the proportion of translated text. Recent literature also highlighted the importance of using open source models for extrinsic data quality values like reproducibility <span class="citation" data-cites="chan2020">(<a href="#ref-chan2020" role="doc-biblioref">Chan et al. 2020</a>)</span>, especially since they perform only slightly less accurate compared to the commercial ones <span class="citation" data-cites="licht2024a">(<a href="#ref-licht2024a" role="doc-biblioref">Licht et al. 2024</a>)</span>. In R, the authors have currently not found any convincing open source alternative integrated in packages. Open source models like OpusMT can be used in Python via <a href="https://huggingface.co/models?pipeline_tag=translation&amp;sort=trending">Hugging Face</a>. In R, these models can be accessed via the <a href="https://rstudio.github.io/reticulate/"><code>reticulate</code></a> package, which offers a connection to Python applications (see <a href="https://rpubs.com/eR_ic/transfoRmers">here</a>). In future, a another option might be the <a href="https://rpubs.com/eR_ic/transfoRmers"><code>text</code></a> package, in which the translation function still has an experimental status.</p>
</div>
</div>
</section>
<section id="minor-text-operations" class="level3">
<h3 class="anchored" data-anchor-id="minor-text-operations">3.3 Minor text operations</h3>
<p>Minor text operations and removing stopwords in text preprocessing are highly dependent on two factors:</p>
<p>a) The text data type</p>
<p>Compared to other types of text data, such as newspaper articles or literature, social media data often features a rather informal communication style and specific characteristics. For instance, the proportion of abbreviations, slang, spelling mistakes, or emojis is usually high, which can be considered as noise (not errors) in the data.</p>
<p>b) The specific method to be applied</p>
<p>Different methods in Computational Social Science require varying strategies of text preprocessing. Methods that use a bag of words approach (BoW), for example, tend to remove a rather high number of different special characters, as these are not regarded as meaningful for interpretation, but rather as disruptive. In contrast, approaches that incorporate context and semantics, such as modern transformer-based models tend to retain characteristics like punctuation marks and generally require less preprocessing steps.</p>
<p>In our case, we will later apply STM, which is based on a bag of word approach. Therefore, we will apply some operations like removing hashtags, punctuaction or URLs. Before we do so, let’s check the length of our documents.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(stringr<span class="sc">::</span><span class="fu">str_count</span>(olympics_data_en_full<span class="sc">$</span>tweet_text, <span class="st">'</span><span class="sc">\\</span><span class="st">w+'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2723</code></pre>
</div>
</div>
<p>First, we will remove all hashtags from the original text and save them in a separate column. After compareing functions from multiple packages, we decided to use the one from <code>textclean</code>, because the ones from the other packages performed slightly less accurate (e.g.&nbsp;some were unable to remove punctuation at the end of an hashtag within a sentence). Removing hashtags from the text is advisable in our case, as we are analyzing tweets that all contain #Olympics2024. In a topic model, #Olympics2024 would likely be closely associated to every topic and would not add significant value to the interpretation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_rem <span class="ot">&lt;-</span> olympics_data_en_full <span class="sc">%&gt;%</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a> dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract hashtags</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">Hashtags =</span> <span class="fu">sapply</span>(<span class="fu">str_extract_all</span>(<span class="st">`</span><span class="at">tweet_text</span><span class="st">`</span>, <span class="st">"#</span><span class="sc">\\</span><span class="st">w+"</span>), paste, <span class="at">collapse =</span> <span class="st">" "</span>),</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Remove hashtags from the original text</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">`</span><span class="at">tweet_text</span><span class="st">`</span> <span class="ot">=</span> textclean<span class="sc">::</span><span class="fu">replace_hash</span>(<span class="st">`</span><span class="at">tweet_text</span><span class="st">`</span>, <span class="at">replacement =</span> <span class="st">""</span>)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        ) <span class="sc">%&gt;%</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Clean up any extra whitespace left after removing hashtags</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="st">`</span><span class="at">tweet_text</span><span class="st">`</span> <span class="ot">=</span> stringr<span class="sc">::</span><span class="fu">str_squish</span>(<span class="st">`</span><span class="at">tweet_text</span><span class="st">`</span>))</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(stringr<span class="sc">::</span><span class="fu">str_count</span>(olympics_data_en_full_rem<span class="sc">$</span>tweet_text, <span class="st">'</span><span class="sc">\\</span><span class="st">w+'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2271</code></pre>
</div>
</div>
<p>Removing the hashtags has reduced our total word count by more than 400 words. In addition to the hashtags, we also want to remove special characters such as URLs, punctuation or usernames as they do not add relevant information in BoW models like LDA or STM. Instead, they rather increase the number of words (tokens) to analyze and therefore the calculation time. Again, we will store the @-mentions in a separate column to preserve information. Finally, as a last step of minor text operations, we will convert the entire text to lowercase in order to ensure the same word is not treated differently due to capitalization.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new column for usernames and then clean the text</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_rem <span class="ot">&lt;-</span> olympics_data_en_full_rem <span class="sc">%&gt;%</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Store the usernames in a new column</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">user_mentions =</span> stringr<span class="sc">::</span><span class="fu">str_extract_all</span>(tweet_text, <span class="st">"@</span><span class="sc">\\</span><span class="st">w+"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">sapply</span>(paste, <span class="at">collapse =</span> <span class="st">", "</span>),  <span class="co"># Extract usernames with '@'</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Clean the text</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">tweet_text =</span> <span class="fu">gsub</span>(<span class="st">"(RT|via)((?:</span><span class="sc">\\</span><span class="st">b</span><span class="sc">\\</span><span class="st">W*@</span><span class="sc">\\</span><span class="st">w+)+)"</span>, <span class="st">""</span>, tweet_text) <span class="sc">%&gt;%</span> <span class="co">#Remove retweets</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">gsub</span>(<span class="st">"https?://</span><span class="sc">\\</span><span class="st">S+"</span>, <span class="st">""</span>, .) <span class="sc">%&gt;%</span>  <span class="co"># Remove URLs</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>      <span class="fu">gsub</span>(<span class="st">"@</span><span class="sc">\\</span><span class="st">w+"</span>, <span class="st">""</span>, .) <span class="sc">%&gt;%</span>          <span class="co"># Remove @usernames from the text</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>      <span class="fu">gsub</span>(<span class="st">"[</span><span class="sc">\r\n</span><span class="st">]"</span>, <span class="st">" "</span>, .) <span class="sc">%&gt;%</span>        <span class="co"># Remove line breaks</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>      <span class="fu">gsub</span>(<span class="st">"[[:punct:]]+"</span>, <span class="st">" "</span>, .) <span class="sc">%&gt;%</span>  <span class="co"># Remove punctuation</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>      <span class="fu">gsub</span>(<span class="st">"</span><span class="sc">\\</span><span class="st">s+"</span>, <span class="st">" "</span>, .) <span class="sc">%&gt;%</span>          <span class="co"># Reduce multiple spaces to a single space</span></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>      <span class="fu">trimws</span>(.) <span class="sc">%&gt;%</span>                     <span class="co"># Trim whitespace from the beginning and end</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>      <span class="fu">tolower</span>()                         <span class="co"># Convert text to lowercase</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(olympics_data_en_full_rem<span class="sc">$</span>tweet_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "the pool is alive with excitement who will claim victory in the competition is fierce"
[2] "the fight against doping is ongoing let s support our athletes by promoting at"       
[3] "the joy of winning a is unmatched who are you supporting at olympics 2024"            
[4] "the intensity of the matches is captivating every match tells a unique story"         
[5] "we must stand united against doping the spirit of fair play should shine at"          
[6] "the culmination of years of effort results in these unforgettable moments"            </code></pre>
</div>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(stringr<span class="sc">::</span><span class="fu">str_count</span>(olympics_data_en_full_rem<span class="sc">$</span>tweet_text, <span class="st">'</span><span class="sc">\\</span><span class="st">w+'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2194</code></pre>
</div>
</div>
</section>
<section id="removing-stopwords" class="level3">
<h3 class="anchored" data-anchor-id="removing-stopwords">3.4 Removing stopwords</h3>
<p>After doing the minor steps of text preprocessing, we now want to focus on removing stopwords as they can highly impact the outcome of certain models. Stopwords are commonly understood to as frequently used words that add only little or no meaning for interpretation. Many popular text mining packages in R like <code>quanteda</code> offer predefined stopword lists. These lists are often applied by default, without considering their specifics and peculiarities. For example, <span class="citation" data-cites="nothman2018">(<a href="#ref-nothman2018" role="doc-biblioref">Nothman, Qin, and Yurchak 2018</a>)</span>identified numerous “surprising omissions (e.g.&nbsp;<em>hasn’t</em> but no <em>hadn’t</em>) and inclusions (e.g.&nbsp;<em>computer</em>)”. Also <span class="citation" data-cites="hvitfeldt2021g">Hvitfeldt and Silge (<a href="#ref-hvitfeldt2021g" role="doc-biblioref">2021b</a>)</span> found out inconsistencies in specific stopword lists. For example, the SMART stopword list includes “he’s” but not “she’s”. Therefore, it is essential to evaluate the impact of stopword lists in relation to the specific text data. In this tutorial, we will compare three commonly used and general stopword lists of the <code>stopwords</code> package: NLTK, SMART and Stopwords ISO. Altough they overlap and can be integrated, <code>stopwords</code> offers a broader selection of different lists compared to general text analysis packages with build-in stopword lists. Let’s compare now the three lists following first the procedure of <span class="citation" data-cites="hvitfeldt2021g">Hvitfeldt and Silge (<a href="#ref-hvitfeldt2021g" role="doc-biblioref">2021b</a>)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">stopwords</span>(<span class="at">source =</span> <span class="st">"nltk"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 179</code></pre>
</div>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">stopwords</span>(<span class="at">source =</span> <span class="st">"smart"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 571</code></pre>
</div>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">stopwords</span>(<span class="at">source =</span> <span class="st">"stopwords-iso"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1298</code></pre>
</div>
</div>
<p>It becomes clear that the lists differ significantly in length. While NLTK is comparatively short, Stopwords ISO contains more than seven times as many words. Let’s now examine which words the three lists would exclude to highlight the differences.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_nltk <span class="ot">&lt;-</span> olympics_data_en_full_rem</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_smart <span class="ot">&lt;-</span> olympics_data_en_full_rem</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_iso <span class="ot">&lt;-</span> olympics_data_en_full_rem</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="co">#function to extract stopwords</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>extract_stopwords <span class="ot">&lt;-</span> <span class="cf">function</span>(text, source) {</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Get stopwords for the specified source</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>     stops <span class="ot">&lt;-</span> stopwords<span class="sc">::</span><span class="fu">stopwords</span>(<span class="at">language =</span> <span class="st">"en"</span>, <span class="at">source =</span> source)</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Split into words</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>     words <span class="ot">&lt;-</span> text <span class="sc">%&gt;%</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>         <span class="fu">strsplit</span>(<span class="st">"</span><span class="sc">\\</span><span class="st">s+"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>         <span class="fu">unlist</span>()</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Find intersection with stopwords</span></span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>     found_stops <span class="ot">&lt;-</span> <span class="fu">intersect</span>(words, stops)</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>     <span class="co"># Return as string</span></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>     <span class="fu">paste</span>(found_stops, <span class="at">collapse =</span> <span class="st">", "</span>)</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a> }</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply extraction for each source</span></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_nltk <span class="ot">&lt;-</span> olympics_data_en_full_nltk <span class="sc">%&gt;%</span></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>     dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>         <span class="at">nltk_stopwords =</span> <span class="fu">sapply</span>(tweet_text, extract_stopwords, <span class="at">source =</span> <span class="st">"nltk"</span>))</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_smart <span class="ot">&lt;-</span> olympics_data_en_full_smart <span class="sc">%&gt;%</span></span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>     dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>         <span class="at">smart_stopwords =</span> <span class="fu">sapply</span>(tweet_text, extract_stopwords, <span class="at">source =</span> <span class="st">"smart"</span>))</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_iso <span class="ot">&lt;-</span> olympics_data_en_full_iso <span class="sc">%&gt;%</span></span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a>     dplyr<span class="sc">::</span><span class="fu">mutate</span>(</span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>         <span class="at">iso_stopwords =</span> <span class="fu">sapply</span>(tweet_text, extract_stopwords, <span class="at">source =</span> <span class="st">"stopwords-iso"</span>))</span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a><span class="co">#number of stopwords (unique stopwords per row)</span></span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(stringr<span class="sc">::</span><span class="fu">str_count</span>(olympics_data_en_full_nltk<span class="sc">$</span>nltk_stopwords, <span class="st">'</span><span class="sc">\\</span><span class="st">w+'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 914</code></pre>
</div>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(stringr<span class="sc">::</span><span class="fu">str_count</span>(olympics_data_en_full_smart<span class="sc">$</span>smart_stopwords, <span class="st">'</span><span class="sc">\\</span><span class="st">w+'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1065</code></pre>
</div>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(stringr<span class="sc">::</span><span class="fu">str_count</span>(olympics_data_en_full_iso<span class="sc">$</span>iso_stopwords, <span class="st">'</span><span class="sc">\\</span><span class="st">w+'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1147</code></pre>
</div>
</div>
<p>As expected, for our artificial data set, the Stopwords ISO list would remove the highest number of words from our text. Even though it contains more than twice as many words as the SMART list, the difference between the two lists is only 83 words. The larger gap is observed between SMART and NLTK. This is likely due to the fact that certain shared stopwords are highly represented across many tweets, while some of the unique words of the respective lists occur relatively rarely. Let’s now apply the three lists and compare the similarity between the procrossed text columns.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>remove_stopwords <span class="ot">&lt;-</span> <span class="cf">function</span>(data, text_column, stopword_source) {</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Get stopwords from the stopwords package</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>  stops <span class="ot">&lt;-</span> stopwords<span class="sc">::</span><span class="fu">stopwords</span>(<span class="at">language =</span> <span class="st">"en"</span>, <span class="at">source =</span> stopword_source)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Remove stopwords from the specified text column</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>  data[[text_column]] <span class="ot">&lt;-</span> <span class="fu">sapply</span>(data[[text_column]], <span class="cf">function</span>(text) {</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    words <span class="ot">&lt;-</span> <span class="fu">strsplit</span>(text, <span class="st">"</span><span class="sc">\\</span><span class="st">s+"</span>)[[<span class="dv">1</span>]]       <span class="co"># Split text into words</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    filtered_words <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(words, stops)     <span class="co"># Remove stopwords</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste</span>(filtered_words, <span class="at">collapse =</span> <span class="st">", "</span>)       <span class="co"># Reassemble text without stopwords</span></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(data)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_nltk <span class="ot">&lt;-</span> <span class="fu">remove_stopwords</span>(olympics_data_en_full_nltk, <span class="st">"tweet_text"</span>, <span class="st">"nltk"</span>)</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_smart <span class="ot">&lt;-</span> <span class="fu">remove_stopwords</span>(olympics_data_en_full_smart, <span class="st">"tweet_text"</span>, <span class="st">"smart"</span>)</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_iso <span class="ot">&lt;-</span> <span class="fu">remove_stopwords</span>(olympics_data_en_full_iso, <span class="st">"tweet_text"</span>, <span class="st">"stopwords-iso"</span>)</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating cosine similarity</span></span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>cosine_similarities_nltk_smart <span class="ot">&lt;-</span> textTinyR<span class="sc">::</span><span class="fu">COS_TEXT</span>(olympics_data_en_full_nltk<span class="sc">$</span>tweet_text, olympics_data_en_full_smart<span class="sc">$</span>tweet_text, <span class="at">separator =</span> <span class="st">" "</span>)</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>cosine_similarities_nltk_iso <span class="ot">&lt;-</span> textTinyR<span class="sc">::</span><span class="fu">COS_TEXT</span>(olympics_data_en_full_nltk<span class="sc">$</span>tweet_text, olympics_data_en_full_iso<span class="sc">$</span>tweet_text, <span class="at">separator =</span> <span class="st">" "</span>)</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>cosine_similarities_smart_iso <span class="ot">&lt;-</span> textTinyR<span class="sc">::</span><span class="fu">COS_TEXT</span>(olympics_data_en_full_smart<span class="sc">$</span>tweet_text, olympics_data_en_full_iso<span class="sc">$</span>tweet_text, <span class="at">separator =</span> <span class="st">" "</span>)</span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cosine_similarities_nltk_smart)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9181705</code></pre>
</div>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cosine_similarities_nltk_iso)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.8658007</code></pre>
</div>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(cosine_similarities_smart_iso)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9386924</code></pre>
</div>
</div>
<p>As expected, the similarity between NLTK and ISO is the lowest, while it is highest between SMART and ISO. Even though the similarities are relatively high, it becomes evident that using specific stopword lists leads to changes in your corpus and subsequently affects your analysis. Therefore, it is crucial to evaluate which lists work best for our data set, use case and the method to be applied. For this reason, we will identify the words that have been removed exclusively by one specific list.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the three stopword variables into one data frame</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>combined_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">nltk_stopwords =</span> olympics_data_en_full_nltk<span class="sc">$</span>nltk_stopwords,</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">smart_stopwords =</span> olympics_data_en_full_smart<span class="sc">$</span>smart_stopwords,</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">iso_stopwords =</span> olympics_data_en_full_iso<span class="sc">$</span>iso_stopwords,</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">stringsAsFactors =</span> <span class="cn">FALSE</span>)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to create a unique word list from a string</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>get_unique_words <span class="ot">&lt;-</span> <span class="cf">function</span>(column) {</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unique</span>(<span class="fu">unlist</span>(stringr<span class="sc">::</span><span class="fu">str_split</span>(column, <span class="st">",</span><span class="sc">\\</span><span class="st">s*"</span>)))  <span class="co"># Split by commas and remove spaces</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sets of unique words for each column</span></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>words_nltk <span class="ot">&lt;-</span> <span class="fu">get_unique_words</span>(combined_df<span class="sc">$</span>nltk_stopwords)</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>words_smart <span class="ot">&lt;-</span> <span class="fu">get_unique_words</span>(combined_df<span class="sc">$</span>smart_stopwords)</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>words_iso <span class="ot">&lt;-</span> <span class="fu">get_unique_words</span>(combined_df<span class="sc">$</span>iso_stopwords)</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Words only in column nltk_stopwords</span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>unique_nltk <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(words_nltk, <span class="fu">union</span>(words_smart, words_iso))</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Words only in column smart_stopwords</span></span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>unique_smart <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(words_smart, <span class="fu">union</span>(words_nltk, words_iso))</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Words only in column iso_stopwords</span></span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>unique_iso <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(words_iso, <span class="fu">union</span>(words_nltk, words_smart))</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the results</span></span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Words only in nltk_stopwords:"</span>, unique_nltk, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Words only in nltk_stopwords:  </code></pre>
</div>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Words only in smart_stopwords:"</span>, unique_smart, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Words only in smart_stopwords:  </code></pre>
</div>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Words only in iso_stopwords:"</span>, unique_iso, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Words only in iso_stopwords: years results line put make work home made proud join top importance end shown opening world making free beginning high year test giving begin important great things give show long </code></pre>
</div>
</div>
<p>It appears that only the ISO list removed unique words. Among these words, we find terms that hold interpretative meaning for our #olympics24 data set such as “results”, “world”, “top”, “proud”, “home”, “show”, “test” or “beginning”. This indicates that Stopwords ISO is not suitable for our use case. Let’s now compare now only NLTK and SMART in order to determine the best option for our analysis. First, we will identify the unique words removed by NLTK but not by SMART, and vice versa.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>get_unique_words_2 <span class="ot">&lt;-</span> <span class="cf">function</span>(column) {</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unique</span>(<span class="fu">unlist</span>(stringr<span class="sc">::</span><span class="fu">str_split</span>(column, <span class="st">",</span><span class="sc">\\</span><span class="st">s*"</span>)))  <span class="co"># Split by commas and remove spaces Leerzeichen</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sets of unique words for each column</span></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>words_nltk <span class="ot">&lt;-</span> <span class="fu">get_unique_words</span>(combined_df<span class="sc">$</span>nltk_stopwords)</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>words_smart <span class="ot">&lt;-</span> <span class="fu">get_unique_words</span>(combined_df<span class="sc">$</span>smart_stopwords)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Words only in column nltk_stopwords</span></span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>unique_nltk <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(words_nltk, words_smart)</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Words only in column smart_stopwords</span></span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>unique_smart <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(words_smart, words_nltk)</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the results</span></span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Words only in nltk_stopwords:"</span>, unique_nltk, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Words only in nltk_stopwords: ve </code></pre>
</div>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Words only in smart_stopwords:"</span>, unique_smart, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Words only in smart_stopwords: let every must first keeps us taken take say last towards comes together truly come behind like never something else best believe nothing new yes possible go way well goes welcome </code></pre>
</div>
</div>
<p>It becomes clear that SMART removes more words without any substantial meaning. However, it still includes some words that could be meaningful for use case, such as first, last, best or together. Additionally, the token “ve” is removed by NLTK but not by SMART. This difference arises because SMART and NLTK use different strategies for handling contradictions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">setdiff</span>(stopwords<span class="sc">::</span><span class="fu">stopwords</span>(<span class="at">source =</span> <span class="st">"nltk"</span>),</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>        stopwords<span class="sc">::</span><span class="fu">stopwords</span>(<span class="at">source =</span> <span class="st">"smart"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "she's"     "that'll"   "don"       "should've" "ll"        "ve"       
 [7] "ain"       "aren"      "couldn"    "didn"      "doesn"     "hadn"     
[13] "hasn"      "haven"     "isn"       "ma"        "mightn"    "mightn't" 
[19] "mustn"     "mustn't"   "needn"     "needn't"   "shan"      "shan't"   
[25] "shouldn"   "wasn"      "weren"     "won"       "wouldn"   </code></pre>
</div>
</div>
<p>NLTK also retains word fragments after removing punctuation like “don”, “ll” or “ve”. Since we have already removed punctuation, we do need to include these forms. A good strategy could therefore be to construct a customized stopword list based on both lists. This would involve incorparating the relevant, unique NLTK words in SMART while excluding words from SMART thate are meaningful for our subject.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co">#safe SMART stopword list as an vector</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>stopwords_smart <span class="ot">&lt;-</span> stopwords<span class="sc">::</span><span class="fu">stopwords</span>(<span class="st">"en"</span>, <span class="at">source =</span> <span class="st">"smart"</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="co">#create additional word lists</span></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>additional_words <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"don"</span>, <span class="st">"ll"</span>, <span class="st">"ve"</span>, <span class="st">"ain"</span>, <span class="st">"aren"</span>, <span class="st">"couldn"</span>, <span class="st">"didn"</span>, <span class="st">"doesn"</span>, <span class="st">"hadn"</span>, <span class="st">"hasn"</span>, <span class="st">"haven"</span>, <span class="st">"isn"</span>, <span class="st">"ma"</span>, <span class="st">"mightn"</span>, <span class="st">"mustn"</span>, <span class="st">"needn"</span>, <span class="st">"shan"</span>, <span class="st">"shouldn"</span>, <span class="st">"wasn"</span>, <span class="st">"weren"</span>, <span class="st">"won"</span>, <span class="st">"wouldn"</span>)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="co">#create a list of word to be removed from SMART</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>words_to_remove <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"against"</span>, <span class="st">"best"</span>, <span class="st">"better"</span>, <span class="st">"first"</span>, <span class="st">"fourth"</span>, <span class="st">"greetings"</span>, <span class="st">"last"</span>, <span class="st">"second"</span>, <span class="st">"third"</span>, <span class="st">"welcome"</span>)</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a><span class="co">#adds additional words</span></span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>stopwords_smart_adapted <span class="ot">&lt;-</span> <span class="fu">unique</span>(<span class="fu">c</span>(stopwords_smart, additional_words))</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a><span class="co">#calculates word counts of SMART stopword list and adapted SMART list after adding words from NLTK</span></span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(stopwords_smart)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 571</code></pre>
</div>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(stopwords_smart_adapted)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 592</code></pre>
</div>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co">#removes words from the words_to_remove list</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>stopwords_smart_adapted <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(stopwords_smart_adapted, words_to_remove)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a><span class="co">#calculates word counts of adapted SMART list after adding and removing specific words</span></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(stopwords_smart_adapted)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 583</code></pre>
</div>
</div>
<p>Let’s now apply our case-specific stopword list to the text.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_smart_adapted <span class="ot">&lt;-</span> olympics_data_en_full_rem</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>remove_stopwords_2 <span class="ot">&lt;-</span> <span class="cf">function</span>(data, text_column, <span class="at">new_column_removed =</span> <span class="st">"stopwords_smart_adapted"</span>) {</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Stopwort-Liste (diese muss zuvor erstellt werden)</span></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>  stops <span class="ot">&lt;-</span> stopwords_smart_adapted</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># new column for removed stopwords</span></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>  data[[new_column_removed]] <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb81-9"><a href="#cb81-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb81-10"><a href="#cb81-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># remove stopwords and save results</span></span>
<span id="cb81-11"><a href="#cb81-11" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">lapply</span>(data[[text_column]], <span class="cf">function</span>(text) {</span>
<span id="cb81-12"><a href="#cb81-12" aria-hidden="true" tabindex="-1"></a>    words <span class="ot">&lt;-</span> <span class="fu">strsplit</span>(text, <span class="st">"</span><span class="sc">\\</span><span class="st">s+"</span>)[[<span class="dv">1</span>]]       <span class="co"># split text in words</span></span>
<span id="cb81-13"><a href="#cb81-13" aria-hidden="true" tabindex="-1"></a>    filtered_words <span class="ot">&lt;-</span> words[words <span class="sc">%in%</span> stops] <span class="co"># found stopwords</span></span>
<span id="cb81-14"><a href="#cb81-14" aria-hidden="true" tabindex="-1"></a>    cleaned_words <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(words, stops)    <span class="co"># text without stopwords</span></span>
<span id="cb81-15"><a href="#cb81-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(</span>
<span id="cb81-16"><a href="#cb81-16" aria-hidden="true" tabindex="-1"></a>      <span class="at">cleaned_text =</span> <span class="fu">paste</span>(cleaned_words, <span class="at">collapse =</span> <span class="st">" "</span>),  <span class="co"># found stopwords as list</span></span>
<span id="cb81-17"><a href="#cb81-17" aria-hidden="true" tabindex="-1"></a>      <span class="at">filtered_words =</span> <span class="fu">paste</span>(filtered_words, <span class="at">collapse =</span> <span class="st">" "</span>) <span class="co"># removed stopwords as list</span></span>
<span id="cb81-18"><a href="#cb81-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb81-19"><a href="#cb81-19" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb81-20"><a href="#cb81-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb81-21"><a href="#cb81-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># save results in column</span></span>
<span id="cb81-22"><a href="#cb81-22" aria-hidden="true" tabindex="-1"></a>  data[[text_column]] <span class="ot">&lt;-</span> <span class="fu">sapply</span>(results, <span class="cf">function</span>(res) res<span class="sc">$</span>cleaned_text)</span>
<span id="cb81-23"><a href="#cb81-23" aria-hidden="true" tabindex="-1"></a>  data[[new_column_removed]] <span class="ot">&lt;-</span> <span class="fu">sapply</span>(results, <span class="cf">function</span>(res) res<span class="sc">$</span>filtered_words)</span>
<span id="cb81-24"><a href="#cb81-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb81-25"><a href="#cb81-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(data)</span>
<span id="cb81-26"><a href="#cb81-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb81-27"><a href="#cb81-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-28"><a href="#cb81-28" aria-hidden="true" tabindex="-1"></a>olympics_data_en_full_smart_adapted <span class="ot">&lt;-</span> <span class="fu">remove_stopwords_2</span>(olympics_data_en_full_rem, <span class="st">"tweet_text"</span>)</span>
<span id="cb81-29"><a href="#cb81-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-30"><a href="#cb81-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-31"><a href="#cb81-31" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(stringr<span class="sc">::</span><span class="fu">str_count</span>(olympics_data_en_full_nltk<span class="sc">$</span>tweet_text, <span class="st">'</span><span class="sc">\\</span><span class="st">w+'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1165</code></pre>
</div>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(stringr<span class="sc">::</span><span class="fu">str_count</span>(olympics_data_en_full_smart_adapted<span class="sc">$</span>tweet_text, <span class="st">'</span><span class="sc">\\</span><span class="st">w+'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1025</code></pre>
</div>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(stringr<span class="sc">::</span><span class="fu">str_count</span>(olympics_data_en_full_smart<span class="sc">$</span>tweet_text, <span class="st">'</span><span class="sc">\\</span><span class="st">w+'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1014</code></pre>
</div>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(stringr<span class="sc">::</span><span class="fu">str_count</span>(olympics_data_en_full_iso<span class="sc">$</span>tweet_text, <span class="st">'</span><span class="sc">\\</span><span class="st">w+'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 932</code></pre>
</div>
</div>
<p>As we can see, our adapted SMART list removed slightly fewer stop words than the original version, despite adding more words than we removed.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>In addition to modifying existing stopword lists, it is also possible to create your own stopword lists based on the specific data set <span class="citation" data-cites="hvitfeldt2021g">(<a href="#ref-hvitfeldt2021g" role="doc-biblioref">Hvitfeldt and Silge 2021b</a>)</span>. Words with a very high frequency (and possibly also those with a very low frequency) are often selected for this purpose. The advantage of this strategy is that the stopword list is created from the use case. However, the determination of threshold values and the inclusion of meaningful words also pose challenges here. In addition to creating corpus-specific stopword lists, some researcher also recommended to perform this step not before but after modeling <span class="citation" data-cites="schofield2017">(<a href="#ref-schofield2017" role="doc-biblioref">Schofield, Magnusson, and Mimno 2017</a>)</span>.</p>
</div>
</div>
<p>What has already been true for minor text operations also applies to stopwords: the choice of a specific stopword list should depend on the data type, the use case and method applied. For example, <span class="citation" data-cites="hvitfeldt2021d">Hvitfeldt and Silge (<a href="#ref-hvitfeldt2021d" role="doc-biblioref">2021a</a>)</span> found with regard to their particular data set and their supervised approach,“the results for all stop word lexicons are worse than removing no stop words at all”. It is also not recommended to use these stopword lists for sentiment analyses, as negations (e.g.&nbsp;not, don’t etc.) are also removed. For STM however, removing stopwords is crucial to increase the model’s interpretability.</p>
</section>
<section id="creating-a-dfm-tokenization-and-lemmatization" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-dfm-tokenization-and-lemmatization">3.6 Creating a DFM: tokenization and lemmatization</h3>
<p>Creating a <code>document-feature-matrix</code> (DFM), or more specifically a <code>document-term-matrix</code>, is a common way to structure text data before analysis. The matrix consists of documents in rows and words in columns displaying the frequency of each word for each document. In order to do so, we first have to tokenize our text data, breaking it into words as smaller sub unit. Before creating such a matrix of tokenized words, it is advisable to lemmatize the words first. Lemmatization refers to the process of merging inflected words into their root form, known as the lemma. In contrast to stemming, which simply removes common suffixes from words, lemmatization results in normalized words. For lemmatization, we will first use the lemma_en.csv list stored in the Git repository. To use the csv file, you have to save it in our local working environment.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>url_3 <span class="ot">&lt;-</span> <span class="st">"https://raw.githubusercontent.com/YannikPeters/DQ_Tool_TextPreprocessing/main/lemma_en.csv"</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>lemma_en <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(url_3)</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>olympics_en_dfm_level3 <span class="ot">&lt;-</span> quanteda<span class="sc">::</span><span class="fu">corpus</span>(olympics_data_en_full_smart_adapted<span class="sc">$</span>tweet_text, <span class="at">docnames =</span> olympics_data_en_full_smart_adapted<span class="sc">$</span>no) <span class="sc">%&gt;%</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># tokenize and remove numbers and symbols</span></span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">tokens</span>(.,<span class="at">remove_numbers=</span><span class="cn">TRUE</span>, <span class="at">remove_symbols =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># lemmatize</span></span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">tokens_replace</span>(lemma_en<span class="sc">$</span>inflected_form, lemma_en<span class="sc">$</span>lemma, </span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">valuetype =</span> <span class="st">"fixed"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># convert to document-feature-matrix</span></span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">dfm</span>() <span class="sc">%&gt;%</span></span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># remove texts that are empty after pre-processing</span></span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">dfm_subset</span>(., <span class="fu">ntoken</span>(.) <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(olympics_en_dfm_level3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 6 documents, 248 features (97.38% sparse) and 0 docvars.
       features
docs    pool alive excitement claim victory competition fierce fight against
  text1    1     1          1     1       1           1      1     0       0
  text2    0     0          0     0       0           0      0     1       1
  text3    0     0          0     0       0           0      0     0       0
  text4    0     0          0     0       0           0      0     0       0
  text5    0     0          0     0       0           0      0     0       1
  text6    0     0          0     0       0           0      0     0       0
       features
docs    dope
  text1    0
  text2    1
  text3    0
  text4    0
  text5    1
  text6    0
[ reached max_nfeat ... 238 more features ]</code></pre>
</div>
</div>
<p>We have now created a DFM for our preprocessed data set. However, it would be valuable to evaluate how the analyses differ when using different levels of preprocessing. In our pipeline so far, we have used approaches of three categories of text preprocessing as defined by <span class="citation" data-cites="churchill2021">Churchill and Singh (<a href="#ref-churchill2021" role="doc-biblioref">2021</a>)</span>: elementary pattern-based preprocessing (e.g.&nbsp;removal of punctuation), dictionary-based preprocessing (e.g.&nbsp;stopword removal), natural language preprocessing (e.g.&nbsp;lemmatization). We will therefore consider it as level 3 preprocessed data. Let’s define three additional preprocessing levels.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Levels</th>
<th>Preprocessing steps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>level 0</td>
<td>tokenization, automated translation</td>
</tr>
<tr class="even">
<td>level 1</td>
<td>tokenization, automated translation and elementary pattern-based preprocessing (removal of punctuation, symbols, hashtags, numbers etc)</td>
</tr>
<tr class="odd">
<td>level 2</td>
<td>tokenization, automated translation, elementary pattern-based preprocessing and dictionary-based preprocessing (stopword removal)</td>
</tr>
<tr class="even">
<td>level 3</td>
<td>tokenization, automated translation, elementary pattern-based preprocessing, dictionary-based preprocessing and natural language preprocessing (lemmatization)</td>
</tr>
</tbody>
</table>
<p>Level 0 leaves the original text mostly unmodified and only uses tokenization. Level 1 then includes all pattern-based preprocessing, but no dictionary-based approaches. In addition to pattern-based preprocessing, level 2 also removes stopwords, but does not use lemmatization. For all levels, we use the combined data set of English tweets and the translated German and French ones.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Be aware that we do not apply all possible preprocessing steps, which might improve topic interpretability as we limited the tutorial to rather “classic” preprocessing. Some researchers, however, also used POS tagging to exclude verbs and some excluded named entities to increase topic interpretability <span class="citation" data-cites="tolochko2024">(<a href="#ref-tolochko2024" role="doc-biblioref">Tolochko et al. 2024</a>)</span>.</p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co">#creating level 0 dfm</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>olympics_en_dfm_level0 <span class="ot">&lt;-</span> quanteda<span class="sc">::</span><span class="fu">corpus</span>(olympics_data_en_full<span class="sc">$</span>tweet_text, <span class="at">docnames =</span> olympics_data_en_full<span class="sc">$</span>no) <span class="sc">%&gt;%</span></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># tokenize</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">tokens</span>(.) <span class="sc">%&gt;%</span></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># convert to document-feature-matrix</span></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">dfm</span>() <span class="sc">%&gt;%</span></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># remove texts that are empty after pre-processing</span></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">dfm_subset</span>(., <span class="fu">ntoken</span>(.) <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a><span class="co">#creating level 1 dfm</span></span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>olympics_en_dfm_level1 <span class="ot">&lt;-</span> quanteda<span class="sc">::</span><span class="fu">corpus</span>(olympics_data_en_full_rem<span class="sc">$</span>tweet_text, <span class="at">docnames =</span> olympics_data_en_full<span class="sc">$</span>no) <span class="sc">%&gt;%</span></span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># tokenize remove numbers and symbols</span></span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">tokens</span>(.,<span class="at">remove_numbers=</span><span class="cn">TRUE</span>, <span class="at">remove_symbols=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># convert to document-feature-matrix</span></span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">dfm</span>()<span class="sc">%&gt;%</span></span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># remove texts that are empty after pre-processing</span></span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">dfm_subset</span>(., <span class="fu">ntoken</span>(.) <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a><span class="co">#creating level 2 dfm</span></span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a>olympics_en_dfm_level2 <span class="ot">&lt;-</span> quanteda<span class="sc">::</span><span class="fu">corpus</span>(olympics_data_en_full_smart_adapted<span class="sc">$</span>tweet_text, <span class="at">docnames =</span> olympics_data_en_full<span class="sc">$</span>no) <span class="sc">%&gt;%</span></span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># tokenize and remove numbers and symbols</span></span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">tokens</span>(.,<span class="at">remove_numbers=</span><span class="cn">TRUE</span>, <span class="at">remove_symbols=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># convert to document-feature-matrix</span></span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">dfm</span>()<span class="sc">%&gt;%</span></span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># remove texts that are empty after pre-processing</span></span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a>  quanteda<span class="sc">::</span><span class="fu">dfm_subset</span>(., <span class="fu">ntoken</span>(.) <span class="sc">&gt;</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s now compare the DFMs with regard to their general descriptive statistics.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>dfms <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Level 0"</span> <span class="ot">=</span> olympics_en_dfm_level0,</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Level 1"</span> <span class="ot">=</span> olympics_en_dfm_level1,</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Level 2"</span> <span class="ot">=</span> olympics_en_dfm_level2,</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Level 3"</span> <span class="ot">=</span> olympics_en_dfm_level3</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a><span class="co"># creating a dataframe</span></span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>dfm_summary <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">Model =</span> <span class="fu">character</span>(),</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">Num_Documents =</span> <span class="fu">numeric</span>(),</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">Unique_Tokens =</span> <span class="fu">numeric</span>(),</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">Total_Tokens =</span> <span class="fu">numeric</span>(),</span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">stringsAsFactors =</span> <span class="cn">FALSE</span></span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a><span class="co"># loop to calculate metrics</span></span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (level <span class="cf">in</span> <span class="fu">names</span>(dfms)) {</span>
<span id="cb92-19"><a href="#cb92-19" aria-hidden="true" tabindex="-1"></a>  dfm <span class="ot">&lt;-</span> dfms[[level]]</span>
<span id="cb92-20"><a href="#cb92-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb92-21"><a href="#cb92-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># calculating dimensions of the DFM</span></span>
<span id="cb92-22"><a href="#cb92-22" aria-hidden="true" tabindex="-1"></a>  num_documents <span class="ot">&lt;-</span> <span class="fu">dim</span>(dfm)[<span class="dv">1</span>]</span>
<span id="cb92-23"><a href="#cb92-23" aria-hidden="true" tabindex="-1"></a>  unique_tokens <span class="ot">&lt;-</span> <span class="fu">dim</span>(dfm)[<span class="dv">2</span>]</span>
<span id="cb92-24"><a href="#cb92-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb92-25"><a href="#cb92-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># calculating total number of tokens</span></span>
<span id="cb92-26"><a href="#cb92-26" aria-hidden="true" tabindex="-1"></a>  total_tokens <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">colSums</span>(dfm))</span>
<span id="cb92-27"><a href="#cb92-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb92-28"><a href="#cb92-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># add data to dataframe</span></span>
<span id="cb92-29"><a href="#cb92-29" aria-hidden="true" tabindex="-1"></a>  dfm_summary <span class="ot">&lt;-</span> <span class="fu">rbind</span>(</span>
<span id="cb92-30"><a href="#cb92-30" aria-hidden="true" tabindex="-1"></a>    dfm_summary,</span>
<span id="cb92-31"><a href="#cb92-31" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.frame</span>(</span>
<span id="cb92-32"><a href="#cb92-32" aria-hidden="true" tabindex="-1"></a>      <span class="at">Model =</span> level,</span>
<span id="cb92-33"><a href="#cb92-33" aria-hidden="true" tabindex="-1"></a>      <span class="at">Num_Documents =</span> num_documents,</span>
<span id="cb92-34"><a href="#cb92-34" aria-hidden="true" tabindex="-1"></a>      <span class="at">Unique_Tokens =</span> unique_tokens,</span>
<span id="cb92-35"><a href="#cb92-35" aria-hidden="true" tabindex="-1"></a>      <span class="at">Total_Tokens =</span> total_tokens</span>
<span id="cb92-36"><a href="#cb92-36" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb92-37"><a href="#cb92-37" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb92-38"><a href="#cb92-38" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb92-39"><a href="#cb92-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-40"><a href="#cb92-40" aria-hidden="true" tabindex="-1"></a><span class="co"># print descriptive statistics</span></span>
<span id="cb92-41"><a href="#cb92-41" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(dfm_summary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    Model Num_Documents Unique_Tokens Total_Tokens
1 Level 0           150           442         2918
2 Level 1           150           380         2174
3 Level 2           150           299         1005
4 Level 3           150           248         1005</code></pre>
</div>
</div>
<p>While the number of tweets remains unaffected by preprocessing at 150, the vocabulary in terms of unique words is significantly reduced by about 44% from level 0 to level 3. With regard to the absolute number of tokens, it is even 65%. It is noticeable that level 2 and 3 are identical in the total number of tokens. This is because all words are replaced during lemmatization, while only the number of unique tokens is reduced. It also becomes clear that the number of tokens differs from what we counted at the beginning with <code>str_count</code>. This is due to the fact that the <code>tokens</code>-function from <code>quanteda</code> has a different strategy to identify tokens. For example, punctuation marks or numbers are also considered as individual tokens by <code>tokens</code> function. Compared to other packages, this approach is well-suited for our purpose of comparing different degrees of preprocessing. For example, the <code>unnest_token</code> function of the <code>tidytext</code> package automatically removes punctuation when tokenizing. However, since we also want to include a model without elementary pattern-based preprocessing for our comparison, this is ideal. Given that the token function from <code>quanteda</code> allows the user to customize the process and is open for fine-tuning (e.g.&nbsp;by using <code>remove_punct</code> etc.), it is preferable for our use case.</p>
</section>
<section id="topic-modeling" class="level3">
<h3 class="anchored" data-anchor-id="topic-modeling">3.7 Topic Modeling</h3>
<p>As mentioned earlier, we will now use STM <span class="citation" data-cites="roberts2019">(<a href="#ref-roberts2019" role="doc-biblioref">Roberts, Stewart, and Tingley 2019</a>)</span> as a topic modeling strategy to analyze our text data. STM can be considered an extensions of LDA <span class="citation" data-cites="blei2002">(<a href="#ref-blei2002" role="doc-biblioref">Blei, Ng, and Jordan 2002</a>)</span>. In contrast to LDA, STM allows the correlation of topics as well as the inclusion of meta-variables in the analysis, which makes it particularly popular in the social sciences. Even though there are newer methods like <a href="https://maartengr.github.io/BERTopic/index.html">BertTopic</a>, they usually do not <a href="https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#document-length">require</a> classic preprocessing steps like stopword removal in advance, but rather after calculating embeddings and clustering documents. At the same time bag of word models still have proven to be effective for determining the effects of text preprocessing on social media data <span class="citation" data-cites="churchill2021">(<a href="#ref-churchill2021" role="doc-biblioref">Churchill and Singh 2021</a>)</span> <span class="citation" data-cites="applest2021">(<a href="#ref-applest2021" role="doc-biblioref">Harrando, Lisena, and Troncy 2021</a>)</span>. Since the focus here is on data quality and not the method itself, we will not delve into the regular steps and best practices for its application.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A more detailed discussion on how to perform topic modeling in the social sciences can be found in <span class="citation" data-cites="maier2018">Maier et al. (<a href="#ref-maier2018" role="doc-biblioref">2018</a>)</span>.</p>
</div>
</div>
<p>Usually, an important step in applying STM is to find the optimal number of topics (k) using certain metrics and manual evaluation. As we are calculating different models based on different text data, we may not end up with the same number of topics per model. As our analysis, however, is based on an artificial data set, we were able to address this issue in advance. We have already integrated the following 10 topics into the data set.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(olympics_data<span class="sc">$</span>subtopic)) <span class="sc">*</span> <span class="dv">100</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Athlete Performances       Basketball Matches         Doping Awareness 
               10.666667                12.000000                11.333333 
      Gold Medal Moments    Gymnastics Highlights            Marathon Race 
               11.333333                 7.333333                12.000000 
Opening/Closing Ceremony        Soccer Excitement          Swimming Events 
               11.333333                 9.333333                 6.000000 
        Wrestling Action 
                8.666667 </code></pre>
</div>
</div>
<p>We can, therefore, directly compare the topics modeled in LDA with the topics we implemented. Because of this, we will use k=10 for all models.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>N<span class="ot">=</span><span class="dv">10</span></span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>dfm_stm0 <span class="ot">&lt;-</span> quanteda<span class="sc">::</span><span class="fu">convert</span>(olympics_en_dfm_level0, <span class="at">to =</span><span class="st">"stm"</span>) </span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>dfm_stm1 <span class="ot">&lt;-</span> quanteda<span class="sc">::</span><span class="fu">convert</span>(olympics_en_dfm_level1, <span class="at">to =</span><span class="st">"stm"</span>) </span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>dfm_stm2 <span class="ot">&lt;-</span> quanteda<span class="sc">::</span><span class="fu">convert</span>(olympics_en_dfm_level2, <span class="at">to =</span><span class="st">"stm"</span>) </span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>dfm_stm3 <span class="ot">&lt;-</span> quanteda<span class="sc">::</span><span class="fu">convert</span>(olympics_en_dfm_level3, <span class="at">to =</span><span class="st">"stm"</span>) </span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1000</span>)</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>STM_0 <span class="ot">&lt;-</span> stm<span class="sc">::</span><span class="fu">stm</span>(dfm_stm0<span class="sc">$</span>documents, dfm_stm0<span class="sc">$</span>vocab, <span class="at">K =</span> N, <span class="at">data =</span> dfm_stm0<span class="sc">$</span>meta, <span class="at">max.em.its =</span> <span class="dv">75</span>, <span class="at">init.type =</span> <span class="st">"LDA"</span>)</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1000</span>)</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>STM_1 <span class="ot">&lt;-</span> stm<span class="sc">::</span><span class="fu">stm</span>(dfm_stm1<span class="sc">$</span>documents, dfm_stm1<span class="sc">$</span>vocab, <span class="at">K =</span> N, <span class="at">data =</span> dfm_stm1<span class="sc">$</span>meta, <span class="at">max.em.its =</span> <span class="dv">75</span>, <span class="at">init.type =</span> <span class="st">"LDA"</span>)</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1000</span>)</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a>STM_2 <span class="ot">&lt;-</span> stm<span class="sc">::</span><span class="fu">stm</span>(dfm_stm2<span class="sc">$</span>documents, dfm_stm2<span class="sc">$</span>vocab, <span class="at">K =</span> N, <span class="at">data =</span> dfm_stm2<span class="sc">$</span>meta, <span class="at">max.em.its =</span> <span class="dv">75</span>, <span class="at">init.type =</span> <span class="st">"LDA"</span>)</span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1000</span>)</span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>STM_3 <span class="ot">&lt;-</span> stm<span class="sc">::</span><span class="fu">stm</span>(dfm_stm3<span class="sc">$</span>documents, dfm_stm3<span class="sc">$</span>vocab, <span class="at">K =</span> N, <span class="at">data =</span> dfm_stm3<span class="sc">$</span>meta, <span class="at">max.em.its =</span> <span class="dv">75</span>, <span class="at">init.type =</span> <span class="st">"LDA"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this tutorial, we will use “LDA” (Gibbs sampler) as an initialization method, even though spectral intitialization is <a href="https://github.com/bstewart/stm/issues/133#issuecomment-357765115">often preferred</a>. Spectral initialization is a deterministic approach, so that random seeds have no impact. However, spectral initialization might lead to slightly <a href="https://github.com/bstewart/stm/blob/dbabf3405c660452bffc8bd1aaf72f7ea3867319/R/stm.R#L82-L101">different results</a> depending on the machine used. In order to avoid differences in the tutorial, we will use LDA for initialization to ensure that the results are reproducible. For this data set, we find the topics being modeled with LDA initialization to be even closer to our predefined topics then those modeled via spectral initialization. Still, this does not imply that one should avoid spectral initialization as it is particularly recommended when working with data sets containing a large number of documents. For Stewart it is still the “<a href="https://github.com/bstewart/stm/issues/133#issuecomment-357766361">first thing</a>” to try for a new data set.</p>
</div>
</div>
<p>Let’s inspect the most important terms per topic.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>STM_0_labels <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(stm<span class="sc">::</span><span class="fu">labelTopics</span>(STM_0, <span class="at">n =</span> <span class="dv">7</span>)<span class="sc">$</span>prob))</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>STM_1_labels <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(stm<span class="sc">::</span><span class="fu">labelTopics</span>(STM_1, <span class="at">n =</span> <span class="dv">7</span>)<span class="sc">$</span>prob))</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>STM_2_labels <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(stm<span class="sc">::</span><span class="fu">labelTopics</span>(STM_2, <span class="at">n =</span> <span class="dv">7</span>)<span class="sc">$</span>prob))</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>STM_3_labels <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(stm<span class="sc">::</span><span class="fu">labelTopics</span>(STM_3, <span class="at">n =</span> <span class="dv">7</span>)<span class="sc">$</span>prob))</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>STM_0_labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               V1            V2                 V3             V4            V5
1             and             !                  !              !           the
2   #olympics2024            at                the            the             !
3      #paris2024 #olympics2024                 is  #olympics2024    #paris2024
4 #gymnastics2024           the      #olympics2024              a             ?
5               !    #paris2024         #paris2024 #wrestling2024 #olympics2024
6              of       #soccer           electric          every   #basketball
7               a            is #paris2024marathon     #paris2024           who
             V6       V7                   V8             V9               V10
1             ! olympics                  the              !               the
2      athletes     2024                   to         doping     #olympics2024
3           the    paris                   us  #olympics2024                in
4 #olympics2024      are        #olympics2024    #cleansport                 !
5            is    skill                   of #saynotodoping             their
6         these        . #closingceremony2024 #dopingscandal             let's
7            at       at                    .             to #swimmingolympics</code></pre>
</div>
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>STM_1_labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         V1          V2       V3     V4       V5         V6       V7         V8
1       the          is      let doping      the        the      the         at
2      gold         the        s     to       is         on       is   olympics
3        us          in athletes     in      are         in      are          a
4   journey         who       is    the athletes basketball    paris      paris
5 celebrate        will    their   fair    these          s       at        our
6        to          to  support    and     what       them   during         as
7      made competition      the      a  matches    display electric incredible
          V9        V10
1        the         of
2       will      every
3       hard        and
4        who      story
5       work olympics24
6        and     spirit
7 dedication   olympics</code></pre>
</div>
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>STM_2_labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        V1        V2       V3          V4          V5          V6        V7
1 marathon      gold   doping   celebrate     promote     support  athletes
2 swimming      made   spirit        work        fair  dedication      best
3  runners   journey  against     winning  basketball     display    counts
4    paris unmatched olympics        hard      doping  gymnastics heartfelt
5    skill   nations     time      thrill   integrity       tells    filled
6   limits    soccer     true competition       sport athleticism   journey
7  pushing     medal   sports       medal competition     routine  cheering
             V8         V9         V10
1       moments   olympics       paris
2      electric     talent   intensity
3      ceremony basketball       story
4         arena   showcase captivating
5       closing  endurance  olympics24
6        energy   favorite     matches
7 unforgettable     making   wrestling</code></pre>
</div>
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>STM_3_labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          V1        V2        V3         V4       V5            V6        V7
1 basketball      gold      dope      arena     make      ceremony   athlete
2    display     paris     sport     soccer marathon        moment   victory
3      skill     medal      fair     energy    dream         match     fight
4       team   journey   promote excitement   runner unforgettable    thrill
5     talent celebrate   against     nation    paris         close testament
6   showcase       win      play  unmatched    limit         cheer admirable
7       good      step integrity      bring     push          join  platform
          V8         V9         V10
1    olympic dedication competition
2 incredible    support     olympic
3 olympics24       hard        game
4     moment      story      fierce
5      close       work       alive
6       mark      match      excite
7     remind  intensity       claim</code></pre>
</div>
</div>
<p>As we can see, our topics become more meaningful the more preprocessing steps are applied. Especially the models at level 0 and level 1 are not clear as they mostly consists of stopwords like “the”, “of”, “is”, “to” or “at”. Regarding level 2 and 3 we do find overlapping topics of course, but also some differences. A comparison of the topics implemented in advance with the ones in model 3 reveals a number of similarities. For instance, we find topics on doping, gold medalists, ceremonies, basektball and athletic performance. In order to represent topic interpretability numerically, often coders summarize the top terms and assign topic labels. Statistical agreement coefficients such as Krippendorff’s alpha <span class="citation" data-cites="krippendorff2004">(<a href="#ref-krippendorff2004" role="doc-biblioref">Krippendorff 2004</a>)</span> would then be calculated. For the purpose of this tutorial, we won’t calculate topic interpretability with human coders, but only compare key internal metrics like <a href="https://mallet.cs.umass.edu/diagnostics.php">semantic coherence and exclusivity</a>. Semantic coherence refers to the degree of which top terms from the same topic occur in the same document. Exclusivity, on the other hand, refers to the degree of how unique the top terms in each topic are compared to other topics. Usually, the relationship between topic coherence and exclusivity is asymmetrical.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co">#calculating model statistics</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>compute_metrics <span class="ot">&lt;-</span> <span class="cf">function</span>(stm_model, documents, model_name, <span class="at">num_topics =</span> <span class="dv">10</span>) {</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Semantic Coherence</span></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>  semantic_coherence <span class="ot">&lt;-</span> stm<span class="sc">::</span><span class="fu">semanticCoherence</span>(stm_model, documents)</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>  mean_coherence <span class="ot">&lt;-</span> <span class="fu">sum</span>(semantic_coherence) <span class="sc">/</span> num_topics</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Exclusivity</span></span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a>  exclusivity <span class="ot">&lt;-</span> stm<span class="sc">::</span><span class="fu">exclusivity</span>(stm_model)</span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>  mean_exclusivity <span class="ot">&lt;-</span> <span class="fu">sum</span>(exclusivity) <span class="sc">/</span> num_topics</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a>  <span class="co">#return values as a list</span></span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">data.frame</span>(</span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">Model =</span> model_name,</span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">Mean_Coherence =</span> mean_coherence,</span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">Mean_Exclusivity =</span> mean_exclusivity</span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb105-17"><a href="#cb105-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb105-18"><a href="#cb105-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-19"><a href="#cb105-19" aria-hidden="true" tabindex="-1"></a><span class="co"># results for all for STM models</span></span>
<span id="cb105-20"><a href="#cb105-20" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb105-21"><a href="#cb105-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compute_metrics</span>(STM_0, dfm_stm0<span class="sc">$</span>documents, <span class="st">"STM_0"</span>, <span class="at">num_topics =</span> <span class="dv">10</span>),</span>
<span id="cb105-22"><a href="#cb105-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compute_metrics</span>(STM_1, dfm_stm1<span class="sc">$</span>documents, <span class="st">"STM_1"</span>, <span class="at">num_topics =</span> <span class="dv">10</span>),</span>
<span id="cb105-23"><a href="#cb105-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compute_metrics</span>(STM_2, dfm_stm2<span class="sc">$</span>documents, <span class="st">"STM_2"</span>, <span class="at">num_topics =</span> <span class="dv">10</span>),</span>
<span id="cb105-24"><a href="#cb105-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">compute_metrics</span>(STM_3, dfm_stm3<span class="sc">$</span>documents, <span class="st">"STM_3"</span>, <span class="at">num_topics =</span> <span class="dv">10</span>)</span>
<span id="cb105-25"><a href="#cb105-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb105-26"><a href="#cb105-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-27"><a href="#cb105-27" aria-hidden="true" tabindex="-1"></a><span class="co"># create a dataframe with results</span></span>
<span id="cb105-28"><a href="#cb105-28" aria-hidden="true" tabindex="-1"></a>results_df <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, results)</span>
<span id="cb105-29"><a href="#cb105-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-30"><a href="#cb105-30" aria-hidden="true" tabindex="-1"></a><span class="co"># show results</span></span>
<span id="cb105-31"><a href="#cb105-31" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(results_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  Model Mean_Coherence Mean_Exclusivity
1 STM_0      -80.29088         9.241435
2 STM_1     -103.92314         9.296437
3 STM_2     -178.24634         9.449605
4 STM_3     -168.72102         9.477607</code></pre>
</div>
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co"># value range for plot</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>x_min <span class="ot">&lt;-</span> <span class="fu">min</span>(results_df<span class="sc">$</span>Mean_Coherence) <span class="sc">-</span> <span class="dv">3</span></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>x_max <span class="ot">&lt;-</span> <span class="fu">max</span>(results_df<span class="sc">$</span>Mean_Coherence) <span class="sc">+</span> <span class="dv">3</span></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>y_min <span class="ot">&lt;-</span> <span class="fu">min</span>(results_df<span class="sc">$</span>Mean_Exclusivity) <span class="sc">-</span> <span class="fl">0.1</span></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>y_max <span class="ot">&lt;-</span> <span class="fu">max</span>(results_df<span class="sc">$</span>Mean_Exclusivity) <span class="sc">+</span> <span class="fl">0.1</span></span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a><span class="co"># creating a Scatterplot </span></span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">ggplot</span>(results_df, <span class="fu">aes</span>(<span class="at">x =</span> Mean_Coherence, <span class="at">y =</span> Mean_Exclusivity, <span class="at">color =</span> Model)) <span class="sc">+</span></span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">5</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span>       </span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> Model), <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">1.2</span>, <span class="at">hjust =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span> </span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_viridis_d</span>(<span class="at">option =</span> <span class="st">"plasma"</span>, <span class="at">end =</span> <span class="fl">0.9</span>) <span class="sc">+</span> </span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb107-17"><a href="#cb107-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Comparing STM-models"</span>,</span>
<span id="cb107-18"><a href="#cb107-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Topic Coherence"</span>,</span>
<span id="cb107-19"><a href="#cb107-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Topic Exclusivity"</span>,</span>
<span id="cb107-20"><a href="#cb107-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"Model"</span> </span>
<span id="cb107-21"><a href="#cb107-21" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb107-22"><a href="#cb107-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(x_min, x_max) <span class="sc">+</span>                             </span>
<span id="cb107-23"><a href="#cb107-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(y_min, y_max) <span class="sc">+</span>                              </span>
<span id="cb107-24"><a href="#cb107-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>(<span class="at">base_size =</span> <span class="dv">14</span>) <span class="sc">+</span>                   </span>
<span id="cb107-25"><a href="#cb107-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb107-26"><a href="#cb107-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">"right"</span>,                     </span>
<span id="cb107-27"><a href="#cb107-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),         </span>
<span id="cb107-28"><a href="#cb107-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>)         </span>
<span id="cb107-29"><a href="#cb107-29" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-33-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The comparison of the models highlights the impact of our preprocessing steps on these two metrics. For semantic coherence, it is evident that the more preprocessing steps are applied, the less coherent the models become. This is primarily because for model 0 and 1, topics include tokens occuring very frequently in the data set and appear across multiple topics (e.g.&nbsp;stopwords). In contrast, the exclusivity score increases with greater preprocessing as topics are less dominated by highly frequent tokens. We already find some smaller differences between model 0 and 1, highlighting the impact of minor text operations. A great distance occurs between model 1 and model 2, underscoring the substantial effect of stopword removal on these metrics. Even though it is shown for our data set that preprocessing results in lower coherence values, this does not argue against the procedure itself. Rather, it indicates 1) that in text preprocessing a certain level of exclusivity must be reached to model meaningful topics, 2) that topic coherence in the use case of preprocessing does not correspond to topic interpretality and 3) how important the manual interpretation and validation of the topics are. Still, this approach offers a structured strategy for evaluating the effects of preprocessing on topic modeling. Still, especially for smaller data sets, these metrics are sensitive and can vary even with smaller changes in the data, which have to be considered.</p>
</section>
</section>
<section id="discussion" class="level1">
<h1>4. Discussion</h1>
<p>The tutorial has emphasized the importance of evaluating preprocessing steps while suggesting systematic comparison at all levels of the research process to identify optimal solution tailored to the use case. It examined effective approaches within the preprocessing steps, but also compared how different levels of preprocessing affect modeling outcomes. These strategies are essential for making informed decisions during the research process. However, applying specific preprocessing steps depends on the use case, the collected data and the methods applied. For our use case - an artificial data set based on #olympics2024 tweets - we conducted several preprocessing steps including: 1) adressing multilingual content, 2) performing minor text operations, 3) removing stopwords and 4) lemmatization. We compared DeepL and GoogleTranslate as well as different custom stopword lists like NLTK, SMART or Stopwords ISO and discussed multiple strategies for assessing the impact of the different strategies on our text data. However, we did not employ data augmentation strategies like POS tagging and named entity recognition. To evaluate the effects of preprocessing, we compared different stages of preprocessing by modeling four separate STMs. Each model was then manually evaluated for topic interpretability. It is important to note that the deviations from the incorporated topics could also be influenced by the small size of the data set, as modeling 10 topics for such limited data is relatively ambitious. We then calculated different internal metrics like semantic coherence and exclusivity to compare the four levels of preprocessing. This analysis allowed us to pinpoint the effects of every text preprocessing step on the STM models. Our finding revealed that semantic coherence decreased with more preprocessing because stopwords, punctuations and symbols co-occurring frequently were removed, while exclusivity increased. Among the preprocessing steps, the removal of stopwords had the most significant impact on these metrics. One way to achieve a model with higher coherence values might involve removing only #Olympics2024 from the tweets and leave the remaining hashtags in the data set. Since hashtags often also represent thematic contextualizations, this might enhance topic interpretability as well.</p>
<p>At the same time, it is noticeable that small deviations can occur when topic models are recalculated. This can be attributed to two primary factors:</p>
<ol type="1">
<li><p>Translation Variance: Deviations can arise from the inherent variability and nuances of automatic translation, which may slightly alter the content during preprocessing.</p></li>
<li><p>Data Set Characteristics: The small size of the data set - comprising a limited number of documents and a relatively low word count per document—amplifies the impact of minor changes. Even small variations in the text can have a noticeable effect on the resulting topic models.</p></li>
</ol>
<p>These factors underscore the importance of considering data set size and preprocessing consistency when analyzing text data, especially in studies using topic modeling techniques. Of course, the results from this small-scale, artificial data set, are not generalizable, as they heavily depend on the relationship between respective data and concrete preprocessing steps employed. <span class="citation" data-cites="churchill2021">Churchill and Singh (<a href="#ref-churchill2021" role="doc-biblioref">2021</a>)</span> found coherence scores across LDA models to be relatively similar in their comparison, with differences arising more clearly between model types rather than preprocessing steps (without including manual evaluation). Ultimately, it is crucial to evaluate preprocessing steps in a context-sensitive manner, considering their influence on results and their suitability for the data and methods used.</p>
</section>
<section id="literature" class="level1 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">5. Literature</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-blei2002" class="csl-entry" role="listitem">
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2002. <span>“Latent Dirichlet Allocation.”</span> In, 601–8. The MIT Press. <a href="https://doi.org/10.7551/mitpress/1120.003.0082">https://doi.org/10.7551/mitpress/1120.003.0082</a>.
</div>
<div id="ref-chan2020" class="csl-entry" role="listitem">
Chan, Chung-Hong, Jing Zeng, Hartmut Wessler, Marc Jungblut, Kasper Welbers, Joseph W Bajjalieh, Wouter van Atteveldt, and Scott L. Althaus. 2020. <span>“Reproducible Extraction of Cross-Lingual Topics (Rectr).”</span> <em>Communication Methods and Measures</em> 14 (4): 285–305. <a href="https://doi.org/10.1080/19312458.2020.1812555">https://doi.org/10.1080/19312458.2020.1812555</a>.
</div>
<div id="ref-churchill2021" class="csl-entry" role="listitem">
Churchill, Rob, and Lisa Singh. 2021. <span>“textPrep: A Text Preprocessing Toolkit for Topic Modeling on Social Media Data.”</span> <em>Proceedings of the 10th International Conference on Data Science, Technology and Applications</em>, 60–70. <a href="https://doi.org/10.5220/0010559000002993">https://doi.org/10.5220/0010559000002993</a>.
</div>
<div id="ref-applest2021" class="csl-entry" role="listitem">
Harrando, Ismail, Pasquale Lisena, and Raphaël Troncy. 2021. <span>“Apples to Apples: A Systematic Evaluation of Topic Models.”</span> <em>Proceedings of the Conference Recent Advances in Natural Language Processing - Deep Learning for Natural Language Processing Methods and Applications</em>, 483–93. <a href="https://doi.org/10.26615/978-954-452-072-4_055">https://doi.org/10.26615/978-954-452-072-4_055</a>.
</div>
<div id="ref-haukelicht2023" class="csl-entry" role="listitem">
Hauke Licht, and Fabienne Lind. 2023. <span>“Going Cross-Lingual: A Guide to Multilingual Text Analysis.”</span> <em>Computational Communication Research</em> 5 (2): 1. <a href="https://doi.org/10.5117/ccr2023.2.3.lich">https://doi.org/10.5117/ccr2023.2.3.lich</a>.
</div>
<div id="ref-hepp2015" class="csl-entry" role="listitem">
Hepp, Andreas. 2015. <span>“Transcultural Communication,”</span> June. <a href="https://doi.org/10.1002/9781394261390">https://doi.org/10.1002/9781394261390</a>.
</div>
<div id="ref-hidalgo-ternero2021" class="csl-entry" role="listitem">
Hidalgo-Ternero, Carlos Manuel. 2021. <span>“Google Translate Vs. DeepL.”</span> <em>MonTI. Monografías de Traducción e Interpretación</em>, January, 154–77. <a href="https://doi.org/10.6035/monti.2020.ne6.5">https://doi.org/10.6035/monti.2020.ne6.5</a>.
</div>
<div id="ref-hvitfeldt2021d" class="csl-entry" role="listitem">
Hvitfeldt, Emil, and Julia Silge. 2021a. <span>“Regression.”</span> In, 105–54. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9781003093459-9">https://doi.org/10.1201/9781003093459-9</a>.
</div>
<div id="ref-hvitfeldt2021g" class="csl-entry" role="listitem">
———. 2021b. <span>“Stop Words.”</span> In, 37–52. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/9781003093459-4">https://doi.org/10.1201/9781003093459-4</a>.
</div>
<div id="ref-krippendorff2004" class="csl-entry" role="listitem">
Krippendorff, K. 2004. <span>“Reliability in Content Analysis: Some Common Misconceptions and Recommendations.”</span> <em>Human Communication Research</em> 30 (3): 411–33. <a href="https://doi.org/10.1093/hcr/30.3.411">https://doi.org/10.1093/hcr/30.3.411</a>.
</div>
<div id="ref-licht2023a" class="csl-entry" role="listitem">
Licht, Hauke. 2023. <span>“Cross-Lingual Classification of Political Texts Using Multilingual Sentence Embeddings.”</span> <em>Political Analysis</em> 31 (3): 366–79. <a href="https://doi.org/10.1017/pan.2022.29">https://doi.org/10.1017/pan.2022.29</a>.
</div>
<div id="ref-licht2024a" class="csl-entry" role="listitem">
Licht, Hauke, Ronja Sczepanski, Moritz Laurer, and Ayjeren Bekmuratovna. 2024. <span>“No More Cost in Translation: Validating Open-Source Machine Translation for Quantitative Text Analysis.”</span> <a href="http://dx.doi.org/10.31219/osf.io/9trjs">http://dx.doi.org/10.31219/osf.io/9trjs</a>.
</div>
<div id="ref-lind2021a" class="csl-entry" role="listitem">
Lind, Fabienne, Jakob-Moritz Eberl, Olga Eisele, Tobias Heidenreich, Sebastian Galyga, and Hajo G. Boomgaarden. 2021. <span>“Building the Bridge: Topic Modeling for Comparative Research.”</span> <em>Communication Methods and Measures</em> 16 (2): 96–114. <a href="https://doi.org/10.1080/19312458.2021.1965973">https://doi.org/10.1080/19312458.2021.1965973</a>.
</div>
<div id="ref-maier2018" class="csl-entry" role="listitem">
Maier, Daniel, A. Waldherr, P. Miltner, G. Wiedemann, A. Niekler, A. Keinert, B. Pfetsch, et al. 2018. <span>“Applying LDA Topic Modeling in Communication Research: Toward a Valid and Reliable Methodology.”</span> <em>Communication Methods and Measures</em> 12 (2-3): 93–118. <a href="https://doi.org/10.1080/19312458.2018.1430754">https://doi.org/10.1080/19312458.2018.1430754</a>.
</div>
<div id="ref-nothman2018" class="csl-entry" role="listitem">
Nothman, Joel, Hanmin Qin, and Roman Yurchak. 2018. <span>“Stop Word Lists in Free Open-Source Software Packages.”</span> <em>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</em>. <a href="https://doi.org/10.18653/v1/w18-2502">https://doi.org/10.18653/v1/w18-2502</a>.
</div>
<div id="ref-roberts2019" class="csl-entry" role="listitem">
Roberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. 2019. <span>“<span><strong>Stm</strong></span>: An <span><em>R</em></span> Package for Structural Topic Models.”</span> <em>Journal of Statistical Software</em> 91 (2). <a href="https://doi.org/10.18637/jss.v091.i02">https://doi.org/10.18637/jss.v091.i02</a>.
</div>
<div id="ref-schofield2017" class="csl-entry" role="listitem">
Schofield, Alexandra, Måns Magnusson, and David Mimno. 2017. <span>“Pulling Out the Stops: Rethinking Stopword Removal for Topic Models.”</span> <em>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</em>. <a href="https://doi.org/10.18653/v1/e17-2069">https://doi.org/10.18653/v1/e17-2069</a>.
</div>
<div id="ref-sebo2024" class="csl-entry" role="listitem">
Sebo, Paul, and Sylvain de Lucia. 2024. <span>“Performance of Machine Translators in Translating French Medical Research Abstracts to English: A Comparative Study of DeepL, Google Translate, and CUBBITT.”</span> Edited by Iftikhar Ahmed Khan. <em>PLOS ONE</em> 19 (2): e0297183. <a href="https://doi.org/10.1371/journal.pone.0297183">https://doi.org/10.1371/journal.pone.0297183</a>.
</div>
<div id="ref-sen2021" class="csl-entry" role="listitem">
Sen, Indira, Fabian Flöck, Katrin Weller, Bernd Weiß, and Claudia Wagner. 2021. <span>“A Total Error Framework for Digital Traces of Human Behavior on Online Platforms.”</span> <em>Public Opinion Quarterly</em> 85 (S1): 399–422. <a href="https://doi.org/10.1093/poq/nfab018">https://doi.org/10.1093/poq/nfab018</a>.
</div>
<div id="ref-tolochko2024" class="csl-entry" role="listitem">
Tolochko, Petro, Paul Balluff, Jana Bernhard, Sebastian Galyga, Noëlle S. Lebernegg, and Hajo G. Boomgaarden. 2024. <span>“What<span>’</span>s in a Name? The Effect of Named Entities on Topic Modelling Interpretability.”</span> <em>Communication Methods and Measures</em> 18 (4): 349–70. <a href="https://doi.org/10.1080/19312458.2024.2302120">https://doi.org/10.1080/19312458.2024.2302120</a>.
</div>
<div id="ref-devries2018" class="csl-entry" role="listitem">
Vries, Erik de, Martijn Schoonvelde, and Gijs Schumacher. 2018. <span>“No Longer Lost in Translation: Evidence That Google Translate Works for Comparative Bag-of-Words Text Applications.”</span> <em>Political Analysis</em> 26 (4): 417–30. <a href="https://doi.org/10.1017/pan.2018.26">https://doi.org/10.1017/pan.2018.26</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>